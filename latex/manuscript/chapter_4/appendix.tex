\chapter{Appendix of Chapter~\ref{chap:gen-flat-minima}}
\label{ap: genflat-minima}

\begin{noaddcontents}
    \section{Supplementary background}
    \label{sec: supp_background}
    \subsection{Additional details on Poincaré and Log-Sobolev inequalities.}
    \paragraph{Proof of proposition \ref{prop:gibbs_logsob}.}
    
    \begin{proof}
        We define $\P_1$ such that $d\P_1(h)\propto \exp(-V(h) - \gamma \hat{\Risk}_{\S_m}(h))dh$, then note that, by convexity assumption over $\ell_1$, $Hess(V+\gamma\hat{\Risk}_{\S_m}) \succeq \frac{1}{c_{LS}}\mathrm{Id}$. Then, applying \citet[Corollary 2.1]{chafai2004entropies}, we know that $\P_1$ satisfies a Poincaré inequality with constant $c_{LS}(\P)$. 
      
        Finally, defining $\P_2$ such that $dP_2(h) \propto \exp(-\frac{\gamma}{m}\sum_{i=1}^m \ell_2(h,\z_i))$, thanks to the boundedness of $\ell_2$, we use \citet[Property 2.6]{guionnet2003lectures}, which ensure that $P_2 = \P_{-\gamma \hat{\Risk}_{\S_m}}d \P_1(h)$ satisfies a Log-Sobolev inequality with constant $2c_{LS}(\P)\exp(4\|\ell_2\|_{\infty}) c_P(\P)$.
        Noting that $\P_2=\P_{-\gamma\hat{\Risk_\S}}$ concludes the proof.
      \end{proof}
    
    
    \paragraph{Proof of \citet[Propsition 2.1]{ledoux2006concentration}}
        We prove here Proposition \ref{prop:ls_implies_poinc}, stated below, showing that Log-Sobolev implies Poincaré.
     \begin{proposition}
        \label{prop:ls_implies_poinc}
        If $\Q$ is $\Lsob(c_{LS})$, then it is also $\Poinc(c_P)$. 
        
        We then have $c_P(\Q)= \frac{c_{LS(\Q)}}{2}$.
      \end{proposition}
        We provide the proof for completeness.
      \begin{proof}
        Let $f\in \mathrm{H}^{1}(\Q)$, such that $\mathbb{E}_{Q}[f]=0$ and $\mathbb{E}[f^2]=1$. For any $\varepsilon>0$, $1+ \varepsilon f \in \mathrm{H}^{1}$. We then apply the Log-Sobolev inequality on $1+\varepsilon f$: 
    
        \begin{align*}
          \mathbb{E}_{Q}\LB (1+ \varepsilon f)^2 \LP 2 \log(1+\varepsilon f) - \log(1+\varepsilon^2)  \RP  \RB \leq c_{LS}(\Q) \varepsilon^2 \mathbb{E}_{\Q}\left[ \|\nabla f\|^2 \right].
        \end{align*}
    
    
        Note that, by a Taylor expansion, $\log(1+ \varepsilon f)= \varepsilon f -\frac{(\varepsilon f)^2}{2} + o\LP \varepsilon^2 \RP$ and also that $ \log(1+ \varepsilon^2)= \varepsilon^2 + o(\varepsilon^2)$. 
        Then, plugging this into the previous equation gives: 
    
        \begin{align*}
          \mathbb{E}_{Q}\LB 2\varepsilon f + 3 (\varepsilon f)^2 - \varepsilon^2 + o(\varepsilon^2)  \RB \leq c_{LS}(\Q) \varepsilon^2 \mathbb{E}_{\Q}\left[ \|\nabla f\|^2 \right].
        \end{align*}
    
        We use that $\mathbb{E}[f]=0$ and we then divide by $\varepsilon^2$. Taking the limit $\varepsilon \rightarrow 0$ gives: 
    
        \begin{align*}
          \mathbb{E}_{Q}\LB 3 f^2 - 1  \RB & \leq c_{LS}(\Q)\mathbb{E}_{\Q}\left[ \|\nabla f\|^2 \right]. 
          \intertext{Using that $\mathbb{E}[f^2]=1$ gives:}
          1 & \leq \frac{c_{LS}(\Q)}{2}\mathbb{E}_{\Q}\left[ \|\nabla f\|^2 \right]  
        \end{align*}
    
        Then, for any $g\in\mathrm{H}^{1}(\Q)$ applying this proof on $f=\frac{g- \mathbb{E}_\Q[g]}{\sqrt{\mathrm{Var}_\Q(g)}}$ concludes the proof.
      \end{proof}
    
    
    
    
    \subsection{Wasserstein distances}
    
    We recall here the definition of Wasserstein distances, valid for any Polish space $\Hcal$ equipped with a distance $d$.
    
    \begin{definition}
    \label{def: wasserstein-chap-4}
    The $1$-Wasserstein distance between $P,Q \in \mathcal{M}(\mathcal{H})^2$ is defined as
    \[ W_1(Q,P) = \inf_{\pi \in \Pi(Q,P)} \int_{\mathcal{H}^2} ||x-y||\mathrm{d}\pi(x,y). \]
    where $\Pi(Q,P)$ denote the set of probability measures on $\mathcal{H}^2$ whose marginals are $Q$ and $P$.
    We define the $2$-Wasserstein distance on $\mathcal{P}(\mathcal{H})$ as
    \[ W_2(Q,P) = \sqrt{\inf_{\pi \in \Pi(Q,P)} \int_{\mathcal{H}^2} ||x-y||^2\mathrm{d}\pi(x,y)}. \]
    \end{definition}
    \begin{comment}
    \subsection{Stochastic Gradient Descent.}
    \begin{algorithm}[htbp]
      \begin{algorithmic}[1]
      \State \textbf{Input:} Initial point $\mathbf{h}_1=0$, steps $(\eta_t)_{t\geq 1}$, dataset $\S$\\
      \textbf{For $t=1\cdots T$:}
      \State \hspace{0.7cm}Draw $j_t\sim Unif\LP\LC 1 \cdots m\RC \RP$
      \State \hspace{0.7cm}Update $\mathbf{h}_{t+1} = \mathbf{h}_t - \eta_t \nabla_h\ell(\mathbf{h}_t,\z_{j_t})$
      \State \textbf{return} $\mathbf{h}_{T+1}$
      \end{algorithmic}
      \caption{Stochastic Gradient Descent}
      \label{alg:sgd}
      \end{algorithm}
    
      \begin{lemma}[Theorem 3.3, \citealp{liL2022sgd}]
      \label{l: sgd_gradient_bounds}
      Assume \textbf{(A1)}. Let $\mathbf{h}_t$ be the iterate produced by Algorithm 1. Assume $\eta_t=$ $\eta_1 t^{-\frac{1}{2}}$ with $\eta_1 \leq 1 /(2 G)$. For any $\delta \in(0,1)$ and uniformly for all $t=1, \ldots T$, with probability $1-\delta$,
    (a) if $\theta=\frac{1}{2}$, then we have the following inequality
    $$
    \begin{aligned}
     \left\|\nabla \Risk_\D\left(\mathbf{h}_{t+1}\right)-\nabla \hat{\Risk}_{\S_m}\left(\mathbf{h}_{t+1}\right)\right\|^2  
     & = \mathcal{O}\left(\frac{T^{\frac{1}{2}} \log ^2\left(\frac{1}{\delta}\right)(\log T)\left(d+\log \left(\frac{1}{\delta}\right)\right)}{m}\right) ;
    \end{aligned}
    $$
    (b) if $\theta \in\left(\frac{1}{2}, 1\right)$ and Assumption \textbf{(A2)} holds, then we have
    $$
    \begin{aligned}
     \left\|\nabla \Risk_\D\left(\mathbf{h}_{t+1}\right)-\nabla \hat{\Risk}_{\S_m}\left(\mathbf{h}_{t+1}\right)\right\|^2 
    & =  \mathcal{O}\left(\frac{T^{\frac{1}{2}} \log ^{2 \theta+1}\left(\frac{1}{\delta}\right)(\log T)\left(d+\log \left(\frac{1}{\delta}\right)\right)}{m}\right) ;
    \end{aligned}
    $$
    (c) if $\theta>1$ and Assumption \textbf{(A2)} holds, then we have
    $$
    \begin{aligned}
    & \left\|\nabla \Risk_\D\left(\mathbf{h}_{t+1}\right)-\nabla \hat{\Risk}_{\S_m}\left(\mathbf{h}_{t+1}\right)\right\|^2 = & \\
    & \mathcal{O}\left(\left(d+\log \left(\frac{1}{\delta}\right)\right)\right. \left.\times \frac{T^{\frac{1}{2}}\left(\log ^{\theta-1}(T / \delta) \log (1 / \delta)+\log ^{(2 \theta+1)}\left(\frac{1}{\delta}\right) \log T\right)}{m}\right)  .
    \end{aligned}
    $$
    \end{lemma}
    \end{comment}
    
    
    \section{PAC-Bayes bounds for Lipschitz losses through log-Sobolev inequalities}
    \label{sec: lpz_results}
    \paragraph{Extending Catoni's bound to Lipschitz losses.}
    A well-known relaxation of \citet[Theorem 1.2.6]{catoni2007pac} (see \eg \citealp[Theorem 4.1]{alquier2016properties}) holding for subgaussian losses has been widely used in practice as a tractable PAC-Bayesian algorithm exhibiting a linear dependency on the KL divergence. We exploit below a consequence of the Herbst argument as stated, \eg,  in \citet[Section 2.3]{ledoux2006concentration}, stating that a $L$-Lipschitz function of a random variable following a distribution $\D$ being $\Lsob(c_{LS})$ is $L\sqrt{c_{LS}(\D)}$ subgaussian. This yields the following corollary. 
    
    \begin{corollary}
      \label{cor: catoni_lpz}
      Let $\lambda >0$, $m\geq 1$ and a data-free prior $\P$. Assume that for any $h\in\Hcal$, $\ell(h,.)$ is $L$-Lipschitz and that the data distribution $\D$ is $\Lsob(c_{LS})$. Then for with probability at least $1-\delta$ over $\S$, for any $\Q\in \Mcal(\Hcal)$,
    
      \[ \Risk_\D(\Q) \leq \hat{\Risk}_{\S_m}(\Q) + \frac{\KL(\Q,\P) + \log(1/\delta)}{\lambda} + \frac{2\lambda^2L^2c_{LS}(\D)}{m}.  \]
    \end{corollary}
     
    \begin{proof}
      We take $f(h)= \lambda \Delta_S(h):= \lambda (\Risk_\D(\Q) - \hat{\Risk}_{\S_m}(\Q))$  first use the change of measure inequality \citep{csizar1975divergence,donsker1976asymp} to state that, for any $\Q$, 
    
      \[ \mathbb{E}_{h\sim\Q}[f(h)] \leq \KL(\Q,\P) + \log \LP\mathbb{E}_{h\sim\P} \LB \exp \LP f(h) \RP \RB \RP. \]
    
      Markov's inequality alongside Fubini's theorem gives, with probability at least $1-\delta$,
    
      \[ \mathbb{E}_{h\sim\Q}[f(h)] \leq \KL(\Q,\P) + \log(1/\delta) + \log \LP\mathbb{E}_{h\sim\P} \mathbb{E}_\S \LB \exp \LP f(h) \RP \RB \RP. \]
    
      Now, we use that $f$ is $L$-Lipschitz for all $h$, then the function $\S \rightarrow \Delta_\S(h)$ is $\frac{2}{\sqrt{m}}$-Lipschitz on $\S$ for each $h$. As $\D$ is $\Lsob(c_{LS})$ inequality, $\D^{\otimes m}$ is also $\Lsob(c_{LS})$ with identical constant \citep[Corollary 3.2.3]{ane2000inegalites}. Then, using Herbst argument similarly as in \citet[Section 2.3]{ledoux2006concentration} allow us to conclude that $f$ is $2L\sqrt{c_{LS}(\D)}$-subgaussian, thus,
    
     \[\log \LP\mathbb{E}_{h\sim\P} \mathbb{E}_\S \LB \exp \LP f(h) \RP \RB \RP \leq \frac{2\lambda^2L^2}{m}.\]
    
     This concludes the proof. 
    \end{proof}
    
    \paragraph{Disintegrated PAC-Bayes bounds} 
    Numerical estimation of PAC-Bayes bounds is usually challenging as it often involves Monte-Carlo approximations of the expectation over the posterior $\Q$.
    A recent line of work developed in \Cref{chap:online-pb} and \citet{rivasplata2020pac,viallard2023general} studies \emph{disintegrated PAC-Bayes bounds} \eg, bounds holding with high-probability on both the dataset $\S$ and a single predictor $h$ drawn from the posterior $\Q$. Those bounds are relevant for practitioners as they require little computational time. However, a drawback of these bounds is that existing disintegrated bounds do not allow the KL divergence to be used as a complexity measure. Either disintegrated KL \citep{rivasplata2020pac} or Rényi divergences \citep{viallard2023general}, which can be seen as a relaxation of the KL one, are considered. 
    
    Using again the subgaussianity behavior of Lipschitz losses, it is possible to attain PAC-Bayesian disintegrated bounds as long as the posterior distribution satisfies a log-Sobolev inequality with sharp constant (achievable for instance for Gaussian distribution with small operator norm).
    
    \begin{lemma}
      \label{l: disintegrated}
      Assume that for any $\z, \ell(.,\z)$ is $L$-Lipschitz and that $\Q$ is $\Poinc(c_P)$ with $c_P(\Q)\leq 1/m$. Then, with probability $1-\delta$ over the draw of $h\sim\Q$:
      \[ \Delta_{\S_m}(h) \leq \Delta_{\S_m}(\Q) + \sqrt{\frac{2L^2\log(1/\delta)}{m}}.  \]
    \end{lemma}
    
    This lemma states that, as long as we assume our loss to be Lipschitz \wrt $h$, then it is possible to easily derive disintegrated PAC-Bayesian bounds. Also notice that Lemma \ref{l: disintegrated} is easily completed by \Cref{cor: catoni_lpz} which makes appear a KL divergence as complexity. Note also that as the loss is Lipschitz, it is also possible to make appear $1$-Wasserstein distance through the bounds of \Cref{chap: wass-pb,chap: wpb-practical}. Thus Having a Log-Sobolev assumption with sharp constant on the posterior distribution is enough to provide disintegrated PAC-Bayesian bounds involving KL or Wasserstein terms instead of Rényi divegerences or disintegrated KL.
    
    \section{Proofs}
    \label{sec: proofs-chap-4}
    \subsection{Proof of Corollary \ref{cor:poincaré_pacb}}
        \label{sec:proof_poincaré_pacb}
        To start this proof, we first state an important intermediary theorem, holding at no assumption on the data-distribution.
      \begin{theorem}
        \label{th:poincaré-pacb}
        For any $C>0$, any $\frac{2}{C}>\lambda >0$, any data-free prior $\P$, any nonnegative loss function $\ell$ such that, for any $\z\in\mathcal{Z}, \ell(.,\z)\in \mathrm{H}^{1}$, and any $\delta\in [0,1]$, we have, with probability at least $1-\delta$ over the sample $\S$, for any $m>0$, any posterior $\Q$ being $\Poinc(c_P)$, such that $\Risk_\D(\Q)\leq C$ and such that for any $\z,\; \ell(.,\z)\in \mathrm{H}^{1}(\Q)$: 
    
        \begin{multline*}
          \Risk_{\D}(\Q) \leq \frac{1}{1-\frac{\lambda C}{2}} \LP \hat{\Risk}_{\S_m}(\Q) + \frac{KL(\Q,\P) +\log(1/\delta)}{\lambda m} \RP \\
           + \frac{\lambda}{2-\lambda C}\LP c_{P}(\Q)\EE_{\z\sim\D} \LB \EE_{h\sim \Q}\LP \|\nabla_h \ell(h,\z)\|^2 \RP \RB  + \Var_{\z\sim\D} \LP \EE_{h\sim\Q}[\ell(h,\z)] \RP  \RP. 
        \end{multline*} 
             
      
      \end{theorem}
    
      Theorem \ref{th:poincaré-pacb} exhibits the influence of the gradient norm of $\nabla_h \ell$ on the generalisation ability: small gradients makes the bound vanish, the remaining variance term is not treated for now and can be assumed bounded, but we cannot then recover a fast rate. We show next that assuming additional assumption over the data distribution circumvent this issue.
    \begin{proof}
        We re-start from \citet[Corollary 17]{chugg2023unified}, for any $\lambda >0$, with probability at least $1-\delta$, for any $m>0$, any posterior $\Q$:
    
        \begin{align*}
          \Risk_{\D}(\Q) \leq  \hat{\Risk}_{\S_m}(\Q) + \frac{\KL(\Q,\P) +\log(1/\delta)}{\lambda m} 
          + \frac{\lambda }{2}\left(   \EE_{h\sim \Q}\left[\mathbb{E}_{\z\sim \D}[\ell (h,\z)^2]  \right]  \right).
        \end{align*}
        Then, the last term is controlled as follows,
        \begin{align*}
          \EE_{h\sim \Q}\left[\mathbb{E}_{\z\sim \D}[\ell (h,\z)^2] \right] &  \leq  \EE_{\z\sim\D} \LB c_{P}(\Q)\EE_{h\sim \Q}\LP \|\nabla_h \ell(h,\z)\|^2 \RP + \LP \EE_{h\sim\Q}[\ell(h,\z)] \RP^2 \RB .
          \intertext{We then make appear a supplementary variance term:}
          & =  \EE_{\z\sim\D} \LB c_{P}(\Q)\EE_{h\sim \Q}\LP \|\nabla_h \ell(h,\z)\|^2 \RP \RB  + \Var_{\z\sim\D} \LP \EE_{h\sim\Q}[\ell(h,\z)] \RP \\
          & \hspace{4mm}+ \LP \EE_{\z\sim\D}\EE_{h\sim\Q}[\ell(h,\z)] \RP^2.
        \end{align*}
    
        Note that by Fubini, the last term on the right-hand side is exactly $ \Risk_\D(\Q)^2$, then using that the averaged true risk is lesser than $C$, and re-organising the terms in \citet[Corollary 17]{chugg2023unified}  gives, for $\lambda \in \LP 0, \frac{2}{C}\RP$:
    
        \begin{multline*}
          \Risk_{\D}(\Q) \leq \frac{1}{1-\frac{\lambda C}{2}} \hat{\Risk}_{\S_m}(\Q) + \frac{KL(\Q,\P) +\log(1/\delta)}{\lambda \LP 1-\frac{\lambda C}{2}\RP m} \\
          + \frac{\lambda}{2-\lambda C}\LP c_{P}(\Q)\EE_{\z\sim\D} \LB \EE_{h\sim \Q}\LP \|\nabla_h \ell(h,\z)\|^2 \RP \RB  + \Var_{\z\sim\D} \LP \EE_{h\sim\Q}[\ell(h,\z)] \RP  \RP.
        \end{multline*}
    
      \end{proof}
    
      Now Theorem \ref{th:poincaré-pacb} is proven, we only need to exploit the Poincaré assumption on the data distribution on the variance term to obtain Corollary \ref{cor:poincaré_pacb}.
    
    \subsection{Proof of Theorem \ref{th:poincaré_grad_lpz}}
    \label{sec: proof_poincaré_grad}
    \begin{proof}
      We start again from Theorem \ref{th:poincaré_gauss}, with $\lambda= 1/C_1$ then have with probability $1-\delta/2$:
    
      \begin{multline}
        \label{eq: fast_rate_grad_lpz_1}
        \Risk_{\D}(\Q) \leq 2 \LP \hat{\Risk}_{\S_m}(\Q) + 2C_1\frac{\KL(\Q,\P) +\log(2/\delta)}{ m} \RP + \frac{c_P(\Q)}{C_1}\EE_{\z\sim\D} \LB \EE_{h\sim \Q}\LP \|\nabla_h \ell(h,\z)\|^2 \RP \RB .
      \end{multline}
    
      We now remark that $g(h,\z):= \|\nabla_h \ell(h,\z)\|^2$ is nonnegative.
       Then, given our assumptions, we apply the route of proof of Theorem \ref{th:poincaré_gauss} on $g$ \ie we start again from the \citep[Corollary 17]{chugg2023unified}, apply Poincaré's inequality on $\Q$ and use the $\texttt{QSB}$ assumption on $g$. We then have for any $\lambda >0$, with probability at least $1-\delta/2$, any $\Q$ being $\Poinc(c_P)$, $\texttt{QSB}\LP g,C_2\RP$ and $g(.,\z)\in \mathrm{H}^{1}(\Q)$ for all $\z$ :
    
       \begin{multline}
        \label{eq: fast_rate_grad_lpz_2}
        \EE_{\z\sim\D} \LB \EE_{h\sim \Q}\LP \|\nabla_h \ell(h,\z)\|^2 \RP \RB \leq \EE_{h\sim \Q}\LB \frac{1}{m}\sum_{i=1}^m \|\nabla_h\ell(h,\z_i)\|^2 \RB  + \frac{\KL(\Q,\P) +\log(2/\delta)}{\lambda m} \\
        + \frac{\lambda c_P(\Q)}{2}\EE_{\z\sim\D} \LB \EE_{h\sim \Q}\LP \|\nabla_h g(h,\z)\|^2 \RP \RB + \frac{\lambda C_2}{2}  \EE_{\z\sim\D} \LB \EE_{h\sim \Q}\LP \|\nabla_h \ell(h,\z)\|^2 \RP \RB.
      \end{multline}
    
      Finally, notice that, by definition of $g$, $\nabla_h g(h,\z) = 2Hess_h(\ell)(h,\z)\nabla_h \ell (h,\z)$, where $Hess_h(\ell)$ denotes the Hessian of $\ell$. Thus, using that $\ell(., \z)$ is $G$ gradient Lipschitz for any $\z$ gives, for any $(h,\z)$ that $\|\nabla_h g(h,\z)\| \leq 2G \|\nabla_h \ell(h,z)\|$. Plugging this in \eqref{eq: fast_rate_grad_lpz_2} gives: 
    
      \begin{multline}
        \label{eq: fast_rate_grad_lpz_3}
        \EE_{\z\sim\D} \LB \EE_{h\sim \Q}\LP \|\nabla_h \ell(h,\z)\|^2 \RP \RB \leq \EE_{h\sim \Q}\LB \frac{1}{m}\sum_{i=1}^m \|\nabla_h\ell(h,\z_i)\|^2 \RB  + \frac{\KL(\Q,\P) +\log(2/\delta)}{\lambda m} \\
        + \frac{\lambda}{2}\LP 4c_P(\Q)G^2 + C_2 \RP  \EE_{\z\sim\D} \LB \EE_{h\sim \Q}\LP \|\nabla_h \ell(h,\z)\|^2 \RP \RB.
      \end{multline}
    
      Finally, using that $c_P(\Q)=c$, taking $\lambda = \frac{1}{4cG^2 + C_2}$ and re-organising the terms in \eqref{eq: fast_rate_grad_lpz_3} gives:
      \begin{multline}
        \label{eq: fast_rate_grad_lpz_4}
        \EE_{\z\sim\D} \LB \EE_{h\sim \Q}\LP \|\nabla_h \ell(h,\z)\|^2 \RP \RB \leq 2\EE_{h\sim \Q}\LB \frac{1}{m}\sum_{i=1}^m \|\nabla_h\ell(h,\z_i)\|^2 \RB  \\+ 2(4cG^2 + C_2)\frac{\KL(\Q,\P) +\log(2/\delta)}{ m} 
      \end{multline}
    
      Finally, taking an union bound and plugging \eqref{eq: fast_rate_grad_lpz_4} in \eqref{eq: fast_rate_grad_lpz_1} concludes the proof.
    \end{proof}
    
    \subsection{Proof of Lemma \ref{l: kl_bound}}
        \label{sec: proof_kl_bound}
        \begin{proof}
      For conciseness, we rename $\Q:=\P_{-\gamma \hat{\Risk}_{\S_m}}$. We first notice that, denoting by $\frac{d\Q}{d\P}$ the Radon-Nikodym derivative of $\Q$ with respect to $\P$: 
    
      \begin{align*}
        \KL\LP \P_{-\gamma \hat{\Risk}_{\S_m}},\P\RP  & = \mathbb{E}_{h\sim \Q} \LB\log\LP\frac{d\Q}{d\P}(h)\RP \RB  \\
        & = \Ent_{\P} \LP\frac{d\Q}{d\P} \RP = \Ent_P[g^2],
      \end{align*}
      where $g =\sqrt{\frac{d\Q}{d\P}}$.
    
      Recall that $\frac{d\Q}{d\P}(h) = \frac{1}{Z}\exp\LP -\gamma \hat{\Risk}_{\S_m}(h)\RP $ where $Z= \EE_{h\sim \P}\LB \exp\LP -\gamma \hat{\Risk}_{\S_m}(h)\RP \RB$.
      
      Then, $g(h) = \frac{1}{\sqrt{Z}}\exp(-\frac{\gamma}{2} \hat{\Risk}_{\S_m}(h)) $ belongs in $\mathrm{H}^{1}(\P)$ as long as $\ell \in \mathrm{H}^{1}$. Indeed, as $\exp$ is infinitely smooth, $g\in D_1(\Rbb^d)$, also as the loss is nonnegative, then $g \leq \frac{1}{\sqrt{Z}}$ thus $g\in L^2(\P)$. Finally, $\nabla g = -\frac{\gamma }{2}g(h)\nabla \hat{\Risk}_{\S_m}(h) $. As $g(h)\leq \frac{1}{\sqrt{K}}$, we only need to bound $ \|\nabla \hat{\Risk}_{\S_m}(h)\|^2$ to ensure that $g\in \mathrm{H}^{1}(\P)$:
    
      \begin{align*}
        \|\nabla \hat{\Risk}_{\S_m}(h)\|^2 & =\frac{1}{m^2}\sum_{1\leq i,j\leq m}\LA \nabla\ell(h,\z_i), \nabla \ell(h,\z_j) \RA \\
        & \leq \frac{1}{2m^2}\sum_{1\leq i,j\leq m} \|\nabla\ell(h,\z_i)\|^2 + \|\nabla\ell(h,\z_j)\|^2 
      \end{align*}
    
      As we assumed $\|\nabla\ell(.,\z)\|^2\in L^2(\P)$ for all $\z$, we conclude that $g\in \mathrm{H}^{1}$.
    
      We then can apply the log-Sobolev inequality to conlude that
    
      \begin{align*}
        \KL\LP \P_{-\gamma \hat{\Risk}_{\S_m}},\P\RP  & \leq c_{LS}(\P) \EE_{h\sim\P}[\|\nabla g(h)\|^2] \\
        & = \frac{\gamma^2 c_{LS}(\P)}{4}\EE_{h\sim\P} \LB \|\nabla_h \hat{\Risk}_{\S_m}(h) \|^2 g^2(h) \RB \\
        &= \frac{\gamma^2 c_{LS}(\P)}{4}\EE_{h\sim\P} \LB \|\nabla_h \hat{\Risk}_{\S_m}(h) \|^2 \frac{d\Q}{d\P}(h) \RB\\
        & =\frac{\gamma^2 c_{LS}(\P)}{4} \EE_{h\sim \P_{-\gamma \hat{\Risk}_{\S_m}}}\LB \|\nabla_h \hat{\Risk}_{\S_m}(h) \|^2 \RB
      \end{align*}
    \end{proof}
      \subsection{Proof of Theorem \ref{th: gibbs_pacb}}
        \label{sec:proof_gibbs_pacb}
      \begin{proof}
      We start again from  \citet[Corollary 17]{chugg2023unified} instantiated with a single $\lambda$, \iid data and a prior $\P$. Then with probability at least $1-\delta$, for any posterior $\Q$ and $m>0$:
    
      \begin{align*}
        \Risk_{\D}(\Q) \leq  \hat{\Risk}_{\S_m}(\Q) + \frac{\KL(\Q,\P) +\log(1/\delta)}{\lambda m} 
        + \frac{\lambda }{2}\left(   \EE_{h\sim \Q}\left[\mathbb{E}_{\z\sim \D}[\ell (h,\z)^2]  \right]  \right),
      \end{align*} 
      where $\z\sim \D$ is independent of $\S$.
    
      For the first inequality, we just take $\lambda =1$, we use that $\ell(h,\z)^2 \leq \ell(h,\z)$ and re-organise the terms. Finally, we upper bound the KL term thanks to Lemma \ref{l: kl_bound}.
    
      For the second inequality, we exploit \Cref{prop:gibbs_logsob} to use the fact that $\P_{-\gamma \hat{\Risk}_{\S_m}}$ is $\Lsob(c_{LS})$ alongside \Cref{prop:ls_implies_poinc} which ensures that $\P_{-\gamma \hat{\Risk}_{\S_m}}$ is $\Poinc(c_P)$ with constant equal to $c_{LS}\LP \P_{-\gamma \hat{\Risk}_{\S_m}}\RP/2$.
    
      We then apply a route of proof similar to Theorem \ref{th:poincaré_gauss}. We have : 
    
      \begin{align*}
        \EE_{h\sim \P_{-\gamma \hat{\Risk}_{\S_m}}}\left[\mathbb{E}_{\z\sim \D}[\ell (h,\z)^2] \right] &  = \EE_{\z\sim\D} \LB \Var_{h\sim \P_{-\gamma \hat{\Risk}_{\S_m}}}\LP \ell(h,\z) \RP + \LP \EE_{h\sim\P_{-\gamma \hat{\Risk}_{\S_m}}}[\ell(h,\z)] \RP^2 \RB 
        \intertext{Applying Poincaré's inequality then gives: }
        & \leq  \EE_{\z\sim\D} \LB c_P(\P)e^{4\|\ell_2\|_{\infty}}\EE_{h\sim \P_{-\gamma \hat{\Risk}_{\S_m}}}\LP \|\nabla_h \ell(h,\z)\|^2 \RP + \LP \EE_{h\sim\P_{-\gamma \hat{\Risk}_{\S_m}}}[\ell(h,\z)] \RP^2 \RB  .
      \end{align*}
    
      Finally, using that $\P_{-\gamma \hat{\Risk}_{\S_m}}$ is $\texttt{QSB}(\ell,C)$ allow us to re-organise the terms as in Theorem \ref{th:poincaré_gauss}. Combining this with Lemma \ref{l: kl_bound} to bound the KL divergence concludes the proof.
      
      
    \end{proof}
    
    
    \subsection{Proof of Theorem \ref{th:2-wass}}
    \label{sec: proof_2-wass}
    \begin{proof}
      We assume first $G=1$. We start from the Kantorovich duality formula \citep[Theorem 5.10]{villani2009optimal} instantiated with the cost function $c(x,y)= \|x-y\|^2$. We have for any $\Q,\P$, because $W_2$ is a distance:
      
     \begin{align}
        \label{eq: kantorovich}
        W^2(\Q,\P) = W^2(\P,\Q) = \sup_{\phi, \psi}\;\mathbb{E}_{h\sim\Q}[\phi(h)] -\mathbb{E}_{h\sim\P}[\psi(h)], 
     \end{align}
      
      where the supremum is taken over the functions $\phi,\psi\in L^1(\Q)\times L^1(\P)$ such that for all $h,h' \in\Hcal^2$, $\phi(h)-\psi(h')\leq \|h-h'\|^2$. 
    
      We claim that if $\phi(h)= f(h) - D\|\nabla f(h)\|$ and $\psi(h') = f(h')$ then the pair $\Phi,\Psi$ satisfies $\phi(h)-\psi(h')\leq \frac{\|h-h'\|^2}{2}$. 
    
      Indeed, 
    
      \begin{align*}
        \phi(h)-\psi(h')& = f(h)-f(h') - D\|\nabla f(h)\|\\
        &  = f\circ g(1)-f\circ g(0) - D\|\nabla f(h)\|,
        \intertext{where $g(t)= th +(1-t)h'$. Then, by the fundamental theorem of calculus, we have }
        \phi(h)-\psi(h') & = \int_{0}^{1} (f\circ g)'(t) dt  - D\|\nabla f(h)\| \\
        & = \int_{0}^{1} \LA \nabla f\LP th+(1-t)h' \RP, h-h' \RA dt - D\|\nabla f(h)\|.
        \intertext{We now control the last term using that $\|h-h'\|\leq D$ and Cauchy-Schwarz:}
        \phi(h)-\psi(h')& \leq \int_{0}^{1} \LA \nabla f\LP th+(1-t)h' \RP, h-h' \RA dt - \LA \nabla f(h), h-h' \RA\\
        & =  \int_{0}^{1} \LA \nabla f\LP th+(1-t)h' \RP - \nabla f(h), h-h' \RA dt.
        \intertext{Then by Cauchy-Schwarz alongside Lipschitz gradient,}
        \phi(h)-\psi(h')& \leq  \|h-h'\|\int_{0}^{1} \LM \nabla f\LP th+(1-t)h' \RP - \nabla f(h)\RM dt \\
        & \leq \|h-h'\|\int_{0}^{1} (1-t) dt\LM h-h'\RM dt\\
        &= \frac{\|h-h'\|^2}{2}.
      \end{align*}
      We then conclude by applying \eqref{eq: kantorovich} to the pair $(2\phi,2\psi)$. The general case with $G\neq 1$ is immediate when considering the pair $(\frac{2}{G}\phi,\frac{2}{G}\psi)$.
    \end{proof}
    
    \subsection{Proof of Corollary \ref{cor:kl-change}}
    \label{sec: proof_kl-change}
    \begin{proof}
      We fix $R>0$ and we start from \Cref{th:2-wass} with predictor space $\Hcal_0=\Bcal(\zerobf,R)$, $f$ being gradient-Lipschitz on this ball and prior and posterior $\Pcal_R\#\Q,\Pcal_R\#\P$,
      
      \[ \mathbb{E}_{h\sim \Q}\LB f\LP\Pcal_R(h)\RP \RB \leq \frac{G}{2}W_2^2\LP\Pcal_R\#\Q,\Pcal_R\#\P \RP + \mathbb{E}_{h\sim \P}\LB f\LP\Pcal_R(h)\RP \RB + 2R\mathbb{E}_{h\sim \Q} \LB \LM\nabla f \LP\Pcal_R(h)  \RP \RM \RB.  \]
    
      We first prove that $W_2^2\LP\Pcal_R\#\Q,\Pcal_R\#\P \RP \leq W_2^2(\Q,\P)$. Let $\pi\in \Gamma(\Q,\P)$ being the optimal transport coupling from $\P$ to $\Q$, \ie 
      \[ W_2^2(\Q,\P)= \mathbb{E}_{(X,Y)\sim \pi} \LB\|X-Y\|^2\RB.   \]
      Then notice that if we denote by $\pi_1= (\Pcal_R,\Pcal_R)\#\pi$, then $\pi_1\in \Gamma\LP \Pcal_R\#\Q, \Pcal_R\#\P\RP$ and so:
    
      \begin{align*}
        W_2^2\LP\Pcal_R\#\Q,\Pcal_R\#\P \RP & \leq \mathbb{E}_{(X,Y)\sim \pi_1}\LB \|X-Y\|^2 \RB \\
        & = \mathbb{E}_{(X,Y)\sim \pi_1}\LB \|\Pcal_R(X)-\Pcal_R(Y)\|^2 \RB.
        \intertext{Using that $\Pcal_R$ is $1$-Lipschitz gives,}
        W_2^2\LP\Pcal_R\#\Q,\Pcal_R\#\P \RP & \leq \mathbb{E}_{(X,Y)\sim \pi_1}\LB \|X-Y\|^2 \RB\\
        & = W_2^2(\Q,\P).
      \end{align*}
    
      Then we need to control $W_2^2(\Q,\P)$. To do so, we use the fact that $\P$ is $\Lsob(c_{LS})$ to affirm, through Otto-Villani's theorem \citep[Theorem 1]{otto2000gene} that the following holds: $W_2^2(\Q,\P)\leq \frac{c_{LS}(\P)}{2}\KL(\Q,\P)$. This concludes the proof. 
    \end{proof}
    
    \subsection{Proof of \Cref{th:wpb-grad}}
    \label{sec:proof_wpb-grad}
    \begin{proof}
        We start from \Cref{th:2-wass}, using that $\Delta_{\S_m}$ is $G$-gradient-Lipschitz for any $m$ to obtain:
        \begin{align*}
            \mathbb{E}_{h\sim \Q}[\Delta_{\S_m}(h)] \leq \frac{G}{2}W_2^2(Q,P) + \mathbb{E}_{h\sim \P}[\Delta_{\S_m}(h)] +D \mathbb{E}_{h\sim\Q}[\|\nabla \Delta_{\S_m} (h)\|]
        \end{align*}
        The only thing left to control is $\mathbb{E}_{h\sim \P}[\Delta_{\S_m}(h)]$. For this, we use that $P$ alongisde the supermartingale concentration inequality of \citet[Corollary 17]{chugg2023unified} instantiated with prior equal to posterior, \iid data showing that, for any $\lambda >0,$ with probability at least $1-\delta$, 
    
        \[\mathbb{E}_{h\sim \P}[\Delta_{\S_m}(h)] \leq \frac{\log(1/\delta)}{\lambda} + \frac{\lambda}{2}\mathbb{E}_{h\sim\P}\Ebb_{\z\sim\D}[\ell(h,z)^2]. \]
        The last term on the right-hand side is bounded by $\sigma^2$ by assumption, then, taking $\lambda= \sqrt{\frac{2\log(1/\delta)}{\sigma^2}}$ gives finally $\mathbb{E}_{h\sim\P}\Ebb_{\z\sim\D}[\ell(h,z)^2] \leq \sqrt{2\log(1/\delta)/m}$, concludes the proof.
    \end{proof}
    
    \begin{comment}
        
    
    \subsection{Proof of Proposition \ref{prop: sgd_bound}}
    \label{sec: proof_sgd_bound}
    \begin{proof}
      We take $\mathbf{h}^* \in \mathrm{argmin}_{h\in\Hcal} \Risk_\D(h)$. We then apply Theorem \ref{th:2-wass} with $\Q,\P$ being the Dirac distributions in $\mathbf{h}_t,\mathbf{h}^*$ and $f= \Risk_\D- \hat{\Risk}_{\S_m}$ which have $2G$-Lipschitz gradients. Re-organising gives
     \begin{align*}
        \Risk_\D(\mathbf{h}_t) - \Risk_\D(\mathbf{h}^*) & \leq \hat{\Risk}_{\S_m}(\mathbf{h}_{T+1}) + G ||\mathbf{h}_{T+1}- \mathbf{h}^*\|^2 -  \hat{\Risk}_{\S_m}(\mathbf{h}^*)  + 2R\|\nabla\Risk_\D(\mathbf{h}_{T+1})- \nabla \hat{\Risk}_{\S_m}(\mathbf{h}_{T+1})\|.
      \end{align*}  
    
      Finally, using that $\hat{\Risk}_{\S_m}(\mathbf{h}_{T+1})-  \hat{\Risk}_{\S_m}(\mathbf{h}^*) \leq 0$ and taking the infimum on $\mathbf{h^*}$ gives,
      
      \begin{align*}
        \Risk_\D(\mathbf{h}_t) - \Risk_\D(\mathbf{h}^*) & \leq  \inf_{\mathbf{h}^* \in \mathrm{argmin}\; \Risk_\D}\;G||\mathbf{h}_{T+1}- \mathbf{h}^*\|^2   + 2R\|\nabla\Risk_\D(\mathbf{h}_{T+1})- \nabla \hat{\Risk}_{\S_m}(\mathbf{h}_{T+1})\|.
      \end{align*} 
      Finally, using that 
      $$\|\nabla\Risk_\D(\mathbf{h}_t)- \nabla \hat{\Risk}_{\S_m}(\mathbf{h}_t)\| = \sqrt{\|\nabla\Risk_\D(\mathbf{h}_t)- \nabla \hat{\Risk}_{\S_m}(\mathbf{h}_t)\|^2},$$
      and applying Lemma \ref{l: sgd_gradient_bounds} concludes the proof.
    
    \end{proof}
    
    \end{comment}
\end{noaddcontents}