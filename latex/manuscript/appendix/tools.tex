\chapter{Some Mathematical Tools}
\addtextlist{loe}{Appendix}

\section{\textsc{Jensen}'s Inequality}

\begin{theorem}[\textsc{Jensen}'s Inequality]
Let $X$ a random variable following a probability distribution $\nu$ with $f$ a real-valued measurable convex function, we have
\begin{align*}
    f\LP\EE_{X\sim\nu}\LB X\RB\RP \le \EE_{X\sim\nu}\Big[ f\LP X\RP \Big].
\end{align*}
\label{ap:tools:theorem:jensen}
\end{theorem}
\begin{noaddcontents}\begin{proof}
Since $f()$ is a convex function, the following inequality holds, \ie, we have
\begin{align*}
\forall X',\quad a\LP X' - \EE_{X\sim\nu}\LB X\RB\RP \le f(X') - f\LP\EE_{X\sim\nu}\LB X\RB\RP,
\end{align*}
where $a$ is the tangent's slope.
By taking the expectation to both sides of the inequality, we have
\begin{align*}
\underbrace{a\LP \EE_{X\sim\nu}\LB X\RB - \EE_{X\sim\nu}\LB X\RB\RP}_{\displaystyle = 0} \le \EE_{X\sim\nu}\LB f(X)\RB - f\LP\EE_{X\sim\nu}\LB X\RB\RP.
\end{align*}
Hence, by rearranging the terms, we prove the claimed result.
\end{proof}\end{noaddcontents}

\section{\textsc{Markov}'s Inequality}
\label{ap:tools:sec:markov}

\begin{theorem}[\textsc{Markov}'s Inequality] Let $X$ a non-negative random variable  following a probability distribution $\nu$ and $\delta>0$, we have
\begin{align*}
    \PP_{X\sim\nu}\LB X\ge \delta\RB \le \frac{\EE_{X\sim\nu}\LB X\RB}{\delta}.
\end{align*}
\label{ap:tools:theorem:first-markov}
\end{theorem}

\begin{noaddcontents}\begin{proof}
First of all, remark that we have the following inequality for any $X$
\begin{align}
    \delta\indic[X \ge \delta] \;\le\; X\indic[X \ge \delta] \;\le\; X.
    \label{ap:tools:eq:proof-general-markov}
\end{align}
Indeed, on the one hand, if $X<\delta$, $\indic[X \ge \delta]=0$, the inequality holds trivially.
On the other hand, if $X\ge\delta$, $\indic[X \ge \delta]=1$ and the inequality becomes $\delta\le X$, which is true.
By taking the expectation of \Cref{ap:tools:eq:proof-general-markov}, we have
\begin{align*}
    \EE_{X\sim\nu}\Big[\delta\indic[X \ge \delta]\Big] \le \EE_{X\sim\nu}\Big[ X\Big].
\end{align*}
From the fact that the expectation of a constant is the constant and by definition of the probability, we have
\begin{align*}
    \delta\PP_{X\sim\nu}\LB X \ge \delta\RB \le \EE_{X\sim\nu}\Big[ X\Big] \quad\iff\quad \PP_{X\sim\nu}\LB X \ge \delta\RB \le \frac{\EE_{X\sim\nu}\LB X\RB}{\delta},
\end{align*}
which is the desired result.
\end{proof}\end{noaddcontents}

\section{Ville's Inequality}

\begin{lemma}[Ville's maximal inequality for supermartingales]
    \label{l: ville_ineq}
    Let $\left(\mathcal{F}_{t}\right)$ be a filtration and $\left(Z_{t}\right)$ a non-negative super-martingale satisfying $Z_{0}=1$ a.s. If $Z_{t}$ is adapted to $\mathcal{F}_{t}$
    and $\mathbb{E}\left[Z_{t} \mid \mathcal{F}_{t-1}\right] \leq Z_{t-1}$ a.s., $t \geq 1$, then, for any $0<\delta<1$, it holds
    $$
    \mathbb{P}\left(\exists T \geq 1: Z_{T}>\delta^{-1}\right) \leq \delta.
    $$
    \end{lemma}
    
    \begin{noaddcontents}\begin{proof}
      We apply the optional stopping theorem \citep[][Thm 4.8.4]{durrett2019probability} with Markov's inequality defining the stopping time $i=\inf \{t>1$ : $\left.Z_{t}>\delta^{-1}\right\}$ so that
      $$
      \mathbb{P}\left(\exists t \geq 1: Z_{t}>\delta^{-1}\right)=\mathbb{P}\left(Z_{i}>\delta^{-1}\right) \leq \mathbb{E}\left[Z_{i}\right] \delta \leq \mathbb{E}\left[Z_{0}\right] \delta \leq \delta.
      $$
    \end{proof}\end{noaddcontents}
    