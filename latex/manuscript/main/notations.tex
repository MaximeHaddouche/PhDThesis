\chapter*{List of Notations}
\addcontentsline{toc}{chapter}{List of Notations}\mtcaddchapter

\def\arraystretch{1.5}

\centerline{\bf General}
\vspace{0.2cm}
\begin{tabular}{cp{13cm}}
$\displaystyle a$ & A scalar (integer or real)\\
$\displaystyle \R$ & The set of real numbers\\
$\displaystyle \R^d$ & The euclidean set of dimension $d$\\
$\|\cdot\|$ & a norm of an euclidean set \\
$\dist\LP\cdot,\cdot\RP$ & A distance on a Polish space. \\ 
$\displaystyle \N$ & The set of natural numbers\\
$\displaystyle \nabla f$ & the gradient of a function $f:\Rbb^d \rightarrow \Rbb$\\
\end{tabular}


\vspace{0.7cm}
\centerline{\bf Statistical Learning Theory}
\vspace{0.2cm}
\begin{tabular}{cp{13cm}}
$\displaystyle \Z$ & Data space. In supervised learning, $\Z=(\X,\Y)$ with $\X,\Y$ input and label spaces \\
$\displaystyle \z$ & A datum of $\Z$, in supervised learning $\z=(\x,\y)$ with $\x$ input and $\y$ label  \\
$\displaystyle \S$ & Learning sample $\S=\{\z_i\}_{i\geq 1}$\\
$\displaystyle \DS$ & Distribution of $\S$\\
$\displaystyle \Sm$ & Restriction of $\S$ to its $m$ first data $\S_m=\{\z_i\}_{i= 1\cdots m}$\\
$\displaystyle \Dm$ & Distribution of $\Sm$\\
$\displaystyle \D$ & For \iid $\S$, distribution of a single datum on $\Z$\\
$\displaystyle \D^m$ & For \iid $\S$, distribution of $\Sm$, \ie $\Dm=\D^m$.\\
$\displaystyle \Tcal$ & For \iid $\S$, Test set drawn from $\D$\\
$\displaystyle \Hcal$ & The set of hypotheses\\
$\displaystyle \h$ & A hypothesis $\h \in \Hcal$\\
$\displaystyle \loss $ & Loss function $\loss : \Hcal\times \Z \rightarrow \Rbb $\\
\end{tabular}

\vspace{0.7cm}

\centerline{\bf Probability Theory}
\vspace{0.2cm}
\begin{tabular}{cp{13cm}}
$\Ebb_{X\sim\nu}\LB\cdot\RB$ & The expectation \wrt the random variable $X\sim\nu$\\
$\Pbb_{X\sim\nu}\LB\cdot\RB$ & The probability \wrt the random variable $X\sim\nu$\\
$\displaystyle \indic\LB a\RB$ & Indicator function; returns $1$ if $a$ is true and $0$ otherwise\\
$(\Fcal_i)_{i\geq 1}$ &Filtration adapted to $\S$ \\
$\Ebb_i[\cdot]$ & Conditional expectation \wrt $\Fcal_i$, \ie $ \Ebb_i[\cdot] := \Ebb\LB\cdot\mid \Fcal_i\RB$\\
$\mathcal{N}(\mu,\Sigma)$ & Gaussian distribution on $\Rbb^d$ with mean $\mu$ and covariance matrix $\Sigma$
\end{tabular}

\vspace{0.7cm}
\centerline{\bf PAC-Bayes framework}
\vspace{0.2cm}
\begin{tabular}{cp{13cm}}
$\Mcal(\H)$ & Set of Probability densities \wrt the reference measure on $\H$\\
$\Q$ & Posterior distribution $\Q\in\Mcal(\H)$ on $\H$\\
$\P$ & Prior distribution $\P\in\Mcal(\H)$ on $\H$\\
$\displaystyle \KL(\Q\|\P)$ & Kullback-Leibler (KL) divergence between $\Q$ and $\P$\\
$\displaystyle D_{\alpha}(\Q\|\P)$ & RÃ©nyi Divergence between $\Q$ and $\P$\\
$\displaystyle \Risk_{\D}(\h)$ & Population Risk of $\h\in\H$ \wrt $\D$, \ie $\Risk_{\D}(\h):= \Ebb_{\z\sim\D}\LB\ell(h,\z)\RB$\\
$\displaystyle \Riskhat_{\Sm}(h)$ & Empirical Risk on $\S_m$, \ie $\displaystyle \Riskhat_{\Sm}(h) \frac{1}{m} \sum_{i=1}^m \ell(h,\z_i)$ \\
$\displaystyle \Delta_{\Sm}(h)$ & Generalisation gap $\Delta_{\Sm}(h):= \Risk_\D(h) - \Riskhat_{\Sm}(h)$ \\
$\Risk_\D(\Q)$ & Expected population risk \wrt $\Q$, \ie $\Risk_\D(\Q) := \Ebb_{h\sim \Q}\LB \Risk_\D(\Q)\RB$\\
$\Riskhat_{\Sm}(\Q)$ & Expected empirical risk \wrt $\Q$, $\Riskhat_{\Sm}(\Q):= \Ebb_{h\sim \Q}\LB \Riskhat_{\Sm}(\Q)\RB$ \\
$\displaystyle \Delta_{\Sm}(\Q)$ & Expected generalisation gap \wrt $\Q$, $\Delta_{\Sm}(\Q):= \Ebb_{h\sim \Q}\LB \Delta_{\Sm}(h) \RB$ \\
$\displaystyle \P_{-f(h)}$ & Gibbs posterior associated to prior $\P$ and function $f: \Hcal \rightarrow \Rbb$
\end{tabular}

\vspace{0.7cm}

\centerline{\bf Optimal transport}
\vspace{0.2cm}
\begin{tabular}{cp{13cm}}
$\W_1$ & The $1$-Wasserstein distance\\
$\W_2$ & The $2$-Wasserstein distance\\
$\Gamma(\Q,\P)$ & Set of all coupling distribution on $\Hcal^2$ whose marginals are $\Q$ and $\P$.
\end{tabular}