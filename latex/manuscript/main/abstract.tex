\section*{Résumés de la thèse}

\paragraph*{Résumé vulgarisé.}
L'apprentissage PAC-Bayésien est une branche de la théorie de l'apprentissage récemment mise en avant pour ses garanties de généralisation des réseaux de neurones profonds permettant de mieux comprendre leur performances empiriques sur des exemples jamais vus par la machine auparavant. Cette théorie a été initialement développée dans le cadre de la théorie de l'information, qui peut s'avérer limitée pour comprendre précisément la capacité de généralisation des réseaux neuronaux profonds, capacité étant acquise via un processus d'optimisation souvent non-exploité dans les bornes PAC-Bayes.  Dans cette thèse, une vision optimisatoire de l'apprentissage PAC-Bayes est proposée et développée à travers de nombreux algorithmes d'apprentissage et de bornes de généralisation, mettant en évidence les différentes interactions entre les bénéfices de la phase d'apprentissage et la généralisation.

\paragraph*{Résumé complet.} 


L'apprentissage PAC-Bayésien est une branche de la théorie de l'apprentissage récemment mise en avant pour ses garanties de généralisation des réseaux de neurones profonds permettant de mieux comprendre leur performances empiriques sur des examples jamais vus par la machine auparavant. Cette théorie a été initialement développée dans le cadre de la théorie de l'information, qui peut s'avérer limitée pour comprendre précisément la capacité de généralisation des réseaux neuronaux profonds, capacité étant acquise via un processus d'optimisation souvent non-exploité dans les bornes PAC-Bayes.  Dans cette thèse, une vision optimisatoire de l'apprentissage PAC-Bayes est proposée et développée à travers de nombreux algorithmes d'apprentissage et de bornes de généralisation, mettant en évidence les différentes interactions entre les bénéfices de la phase d'apprentissage et la généralisation.

En effet, l'apprentissage PAC-Bayes est classiquement développé via la théorie de l'information, impliquant des quantités interprétées comme bayésiennes telles que la connaissance 'a priori'. Cela peut être difficile à concilier avec l'optimisation concrète des réseaux neuronaux profonds, qui implique l'optimisation d'un grand ensemble de paramètres et ne fait pas appel à l'apprentissage Bayésien. Pour combler cette lacune, nous remettons en question les interprétations et les hypothèses du PAC-Bayes issues de la théorie de l'information et proposons une nouvelle perspective basée sur l'optimisation. 

Plus précisément, nous présentons l'apprentissage PAC-Bayes au chapitre 1, ainsi que notre nouvelle vision optimisatoire. Le chapitre 2 remet en question les hypothèses statistiques du PAC-Bayes. Le chapitre 3 introduit l'apprentissage PAC-Bayesien en ligne, qui permet de réduire l'impact de l'initialisation pendant le processus d'apprentissage. Le chapitre 4 atténue l'impact de l'optimisation dans l'apprentissage par lots en exploitant les "minima plats", un certain type de minima souvent atteint par les réseaux neuronaux profonds, ce qui permet de mieux comprendre la généralisation dans de telles structures. Le chapitre 5 montre qu'il est possible de combiner l'apprentissage PAC-Bayes et le transport optimal, ce qui permet d'incorporer directement des garanties d'optimisation dans une borne PAC-Bayes. Enfin, le chapitre 6 constitue un premier pas vers la pratique en mettant en œuvre de nouveaux algorithmes PAC-Bayes en ligne et par tas pour des Diracs, ce qui n'est pas possible lorsque la théorie de l'information est utilisée.

\section*{Thesis summaries}

\paragraph*{Lay summary.}
PAC-Bayesian learning is a branch of learning theory recently highlighted for its tight generalisation guarantees of deep neural networks, yielding a sharper understanding of their practical efficiency on a novel, unseen example. This theory was initially developed through information theory, which may prove to be limiting for understanding precisely the generalisation capacity of deep neural networks, acquired through an optimisation process. Indeed, a large part of the PAC-Bayesian literature does not dwell on the characteristics and positive impact of the learning phase to enrich the understanding of the generalisation phenomenon observed in practice. In this thesis, an optimisation-driven vision of PAC-Bayes learning is proposed and developed via numerous learning algorithms and generalisation bounds, highlighting various interplays between the benefits of the learning phase and generalisation.

\paragraph*{Full summary.}
PAC-Bayesian learning is a branch of learning theory recently highlighted for its tight generalisation guarantees of deep neural networks, yielding a sharper understanding of their practical efficiency on a novel, unseen example. This theory was initially developed through information theory, which may prove to be limiting for understanding precisely the generalisation capacity of deep neural networks, acquired through an optimisation process. Indeed, a large part of the PAC-Bayesian literature does not dwell on the characteristics and positive impact of the learning phase to enrich the understanding of the generalisation phenomenon observed in practice. In this thesis, an optimisation-driven vision of PAC-Bayes learning is proposed and developed via numerous learning algorithms and generalisation bounds, highlighting various interplays between the benefits of the learning phase and generalisation.

Indeed, PAC-Bayes learning is classically developed through an information-theoretic prism involving in particular Bayesian quantities such as prior knowledge. This may be hard to fit with concrete optimisation procedure of deep neural networks, involving optimisation on large parameters set without the Bayesian paradigm. To fill this gap, we challenge the information-theoretic interpretations and assumptions disseminated within the PAC-Bayes literature by proposing an optimisation-based perspective. 

More precisely, we introduce PAC-Bayes learning in Chapter 1, as well as our novel optimisation view. Chapter 2 challenges statistical assumptions of PAC-Bayes. Chapter 3, introduces Online PAC-Bayes Learning, allowing to reduce the impact of the initialisation during the learning process. Chapter 4 attenuates the impact of optimisation in batch learning by exploiting 'flat minima', a certain type of minima often reached by deep neural networks, providing a sharper understanding of generalisation in such structures. Chapter 5 shows that it is possible to mix up PAC-Bayes learning and optimal transport, allowing to directly incorporate optimisation guarantees in a PAC-Bayes generalisation bound. Finally, Chapter 6 is a first step towards practitioners by implementing novel batch and online PAC-Bayes algorithms for Dirac distribution, which is not possible by the information-theoretic approach of PAC-Bayes.