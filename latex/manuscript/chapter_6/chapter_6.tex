%!TEX root = main.tex
\chapter[Wasserstein PAC-Bayes in Practice: Genrealisation-Driven Learning Algorithms for Deterministic Predictors]{Wasserstein PAC-Bayes in Practice: Genrealisation-Driven Learning Algorithms for Deterministic Predictors}
\label{chap: wpb-practical}
\addchapterlof
\addchapterloe

\vspace{-2.0cm}
\begin{center}
\textbf{This chapter is based on the following paper}\\[-0.1cm]
\end{center}
TODO
%\printpublication{ViallardGermainHabrardMorvant2022}

\vspace{0.2cm}
\minitoc 

\begin{abstract}
\vspace{-0.2cm}
After \Cref{chap: wass-pb} which proposed a theoretical study of PAC-Bayes learning with Wasserstein distances, building bridges with the exploiting of convergence guarantees in generalisation, we now focus on practical expansions of Wasserstein PAC-Bayes. The optimisation view of PAC-Bayes learning is deeply exploited here: we derive theory-driven batch and online algorithms (the online paradigm attenuates the impact of the prior) valid for deterministic predictors (and thus consistent with many practical optimisation algorithms) and are derived from bounds valid for heavy-tailed lipschitz losses (weak statistical assumption and a stronger geometric one to be in line with the optimisation literature). This chapter shows that the optimisation view of PAC-Bayes leads to efficient procedures, competing with classical methods.
\end{abstract}

\newpage
    
\section{Introduction}

\Cref{chap: wass-pb} introduced Wasserstein PAC-Bayes learning from a theoretical perspective. Indeed, the main goal there was to incorporate the convergence guarantees of existing algorithms onto a generalisation bound. On the contrary, we focus here on deriving novel learning algorithms from Wasserstein PAC-Bayes bounds, circumventing many classical limitations of KL-based PAC-Bayes, which is the major part of the literature. Indeed, the practical use of KL divergence comes with two main limitations: {\it (i)} as illustrated in the generative modeling literature, the KL divergence does not incorporate the underlying geometry or topology of the data space $\Z$, hence can behave in an erratic way \cite{arjovsky2017wasserstein},
{\it (ii)} the $\KL$ divergence and its variants require the posterior $\Q$ to be absolutely continuous with respect to the prior $\P$.
However, recent studies \citep{camuto2021fractal} have shown that, in stochastic optimisation, the distribution of the iterates, which is the natural choice for the posterior, can converge to a \emph{singular distribution}, which does not admit a density with respect to the Lebesgue measure.
Moreover, the structure of the singularity (\ie, the \emph{fractal dimension} of $\Q$) depends on the data sample $\S$ \citep{camuto2021fractal}. 
Hence, in such a case, it would not be possible to find a suitable prior $\P$ that can dominate $\Q$ for almost every $\S \sim \D^m$, which will trivially make $\KL(\Q\|\P) = +\infty$ and the generalisation bound vacuous. 

Some works have focused on replacing the Kullback-Leibler divergence with more general divergences in PAC-Bayes \citep{alquier2018simpler,ohnishi2021novel,picard2022change}, although the problems arising from the presence of the $\KL$ divergence in the generalisation bounds are actually not specific to PAC-Bayes: information-theoretic bounds \citep{goyal2017pac,xu2017info,russo2020how} also suffer from similar issues as they are based on a mutual information term, which is the $\KL$ divergence between two distributions.
In this context, as a remedy to these issues introduced by the $\KL$ divergence, \cite{zhang2018optimal,wang2019information,rodriguez2021tighter,lugosi2022generalization} proved analogous bounds that are based on the \emph{Wasserstein distance}, which arises from the theory of optimal transport~\cite{monge1781memoire}.
As the Wasserstein distance inherits the underlying geometry of the data space and does not require absolute continuity, it circumvents the problems introduced by the $\KL$ divergence.
Yet, these bounds hold only in expectation, \ie, none of these bounds is holding with high probability over the random choice of the learning sample $\S\sim\D^{m}$.

In the context of PAC-Bayesian learning, the recent works \cite{amit2022integral,chee2021learning} incorporated Wasserstein distances as a complexity measure and proved generalisation bounds based on the Wasserstein distance.
More precisely, \cite{amit2022integral} proved a high-probability generic PAC-Bayesian bound for bounded losses depending on an integral probability metric \citep{muller1997integral}, which contains the Wasserstein distance as a special case. 
On the other hand, \cite{chee2021learning} exploited PAC-Bayesian tools to obtain learning strategies with their associated regret bounds based on the Wasserstein distance for the \emph{online learning} setting while requiring a finite hypothesis space and do not deal with generalisation.

\textbf{Contributions.}
The theoretical understanding of the high-probability generalisation bounds based on the Wasserstein distance is still limited.
The aim of this paper is not only to prove generalisation bounds (for different learning settings) based on the optimal transport theory but also to propose new learning algorithms derived from our theoretical results.
\begin{enumerate}[label={\it (\roman*)}]
    \item Using the supermartingale toolbox introduced in \Cref{chap: pb-ht}, we prove in \Cref{sec:wasserstein-batch}, novel PAC-Bayesian bounds based on the Wasserstein distance for \iid data.
    While \cite{amit2022integral} proposed a McAllester-like bound for bounded losses, we propose a Catoni-like bound (see \eg, \citealp[Theorem 4.1]{alquier2016properties}) valid for heavy-tailed losses with bounded order 2 moments.
    This assumption is less restrictive than assuming subgaussian or bounded losses, which are at the core of many PAC-Bayes results.
    This assumption also covers distributions beyond subgaussian or subexponential ones (\eg, gamma distributions with a scale smaller than 1, which have an infinite exponential moment). 
    \item We provide in \Cref{sec:wasserstein-online} the first generalisation bounds based on Wasserstein distances for the online PAC-Bayes framework of \Cref{chap:online-pb}.
    Our results are, again, Catoni-like bounds and hold for heavy-tailed losses with bounded order 2 moments.
    Previous work \citep{chee2021learning} already provided online strategies mixing PAC-Bayes and Wasserstein distances.
    However, their contributions focus on the best deterministic strategy, regularised by a Wasserstein distance, with respect to the deterministic notion of regret.
    Our results differ significantly as we provide the best-regularised strategy (still in the sense of a Wasserstein term) with respect to the notion of generalisation, which is new.
    \item As our bounds are linear with respect to Wasserstein terms (contrary to those of \citealp{amit2022integral} and \Cref{chap: wass-pb}), they are well suited for optimisation procedures.
    Thus, we propose the first PAC-Bayesian learning algorithms based on Wasserstein distances instead of KL divergences.
    For the first time, we design PAC-Bayes algorithms able to output deterministic predictors (instead of distributions over all $\H$) designed from deterministic priors.
    This is due to the ability of the Wasserstein distance to measure the discrepancy between Dirac distributions.     
    We then instantiate those algorithms in \Cref{sec:experiments} on various datasets, paving the way to promising practical developments of PAC-Bayes learning. 
\end{enumerate}

To sum up, we highlight two benefits of PAC-Bayes learning with Wasserstein distance.
First, it ships with sound theoretical results exploiting the geometry of the predictor space, holding for heavy-tailed losses.
Such a weak assumption on the loss extends the usefulness of PAC-Bayes with Wasserstein distances to a wide range of learning problems, encompassing bounded losses.
Second, it allows us to consider deterministic algorithms (\ie, sampling from Dirac measures) designed with respect to the notion of generalisation: we showcase their performance in our experiments.

\textbf{Outline.} \Cref{sec:framework} describes our framework and background, \Cref{sec:wasserstein} contains our new theoretical results and \Cref{sec:experiments} gathers our experiments. 
\Cref{sec:discussion-supervised} gathers supplementary discussion, \Cref{sec:proofs} contains all proofs of our claims, and \Cref{sec:supplementary-expes} provides insights into our practical results as well as additional experiments.

\section{Our framework}
\label{sec:framework}

\textbf{Framework.} 
We consider a Polish predictor space $\H$ equipped with a distance $d$ and a $\sigma$-algebra $\Sigma_{\Hcal}$, a data space $\Z$, and a loss function $\loss : \H\times \Z \rightarrow \R$.
In this work, we consider Lipschitz functions with respect to $d$.
We also associate a filtration $(\Fcal_{i})_{i\geq 1}$ adapted to our data $(\z_i)_{i=1,\dots,m}$, and we assume that the dataset $\S$ follows the distribution $\Dcal_{\S}$.
In PAC-Bayes learning, we construct a data-driven posterior distribution $\Q\in\Mcal(\H)$ with respect to a prior distribution $\P$. 

\textbf{Definitions.} 
For all $i$, we denote by $\EE_{i}[\cdot]$ the conditional expectation $\EE[\ \cdot\mid \Fcal_i]$.
In this work, we consider data-dependent priors.
A stochastic kernel is a mapping $\P: \cup_{m=1}^\infty\Z^m\times \Sigma_{\Hcal} \rightarrow [0,1]$ where {\it (i)} for any $B\in \Sigma_{\Hcal}$, the function  $\S\mapsto \P(\S,B)$ is measurable, {\it (ii)} for any dataset $\S$, the function $B\mapsto \P(\S,B)$ is a probability measure over $\H$.

In what follows, we consider two different learning paradigms: \emph{batch learning}, where the dataset is directly available, and \emph{online learning}, where data streams arrive sequentially.

\textbf{Batch setting.} 
We assume the dataset $\Sm$ to be \iid, so there exists a distribution $\D$ over $\Z$ such that $\Dcal_{\Sm}=\D^m$.
We then define, for a given $h\in\H$, the \emph{risk} to be $\Risk_\D\defeq\EE_{\z\sim \D}[\loss(h,\z)]$ and its empirical counterpart $\Riskhat_{\Sm} \defeq \frac{1}{m}\sum_{i=1}^m \loss(h,\z_i)$. 
Our results aim to bound the \emph{expected generalisation gap} defined by $\EE_{h\sim\Q}[ \Risk_{\D}(h) - \Riskhat_{\Sm}(h)]$.
We assume that the dataset $\S$ is split into $K$ disjoint sets $\S_1,\dots,\S_K$.
We consider $K$ stochastic kernels  $\P_1,\dots,\P_K$ such that for any $\S$, the distribution $\P_{i}(\S,.)$ {\it does not} depend on $\S_i$.

\textbf{Online setting.} 
We adapt the online PAC-Bayes framework of \Cref{chap:online-pb}.
We assume that we have access to a stream of data $\S=(\z_i)_{i\geq1}$, arriving sequentially, with no assumption on $\Dcal_{\S}$.
In online PAC-Bayes, the goal is to define a posterior sequence $(\Q_i)_{i\geq 1}$ from a prior sequence $(\P_i)_{i\geq 1}$, which can be data-dependent.
We assume our sequence of stochastic kernels (used as priors) $(\P_i)_{i=1\cdots m}$ to satisfy: {\it (i)} for all $i$ and dataset $\S$, the distribution $\P_i(S,.)$ is $\Fcal_{i-1}$ measurable and {\it (ii)} there exists $\P_0$ such that for all $i \geq 1$, we have $\P_i(S,.)\ll \P_{0}$. Indeed, all those measures are uniformly continuous with respect to any Gaussian distribution.
This last condition covers, in particular, the case where $\H$ is an Euclidean space and for any $i$, the distribution $\P_{i,\S}$ is a Dirac mass. This is weaker than the condition \textit{(ii)} of the online predictive sequence in \Cref{chap:online-pb}, but enough to exploit the conditional Fubini lemma (\Cref{l: cond_fubini-chap3}).
 

\textbf{Wasserstein distance.}
We focus on the Wasserstein distance of order 1 introduced by \cite{kantorovich1960mathematical} in the optimal transport literature. 
Given a distance $d: \Acal\times\Acal \to \Rbb$ and a Polish space $(\Acal, d)$, for any probability measures $\alpha$ and $\beta$ on $\Acal$, the Wasserstein distance is defined by
\begin{align}
\W(\alpha, \beta) \defeq \inf_{\gamma\in \Gamma(\alpha, \beta)}\
\EE_{(a, b)\sim\gamma}d(a, b),\label{eq:wasserstein}
\end{align}
where $\Gamma(\alpha, \beta)$ is the set of joint probability measures $\gamma \in \Mcal(\Acal^2)$ such that the marginals are $\alpha$ and $\beta$.
The Wasserstein distance aims to find the probability measure $\gamma\in\Mcal(\Acal^2)$ minimising the expected cost $\EE_{(a, b)\sim\gamma}d(a, b)$.
We refer the reader to \cite{villani2009optimal,peyre2019computational} for an introduction to optimal transport.

\section{Wasserstein-based PAC-Bayesian generalisation bounds}
\label{sec:wasserstein}

We present novel high-probability PAC-Bayesian bounds involving Wasserstein distances instead of the classical Kullback-Leibler divergence. 
Our bounds hold for heavy-tailed losses (instead of classical subgaussian and subexponential assumptions), extending the remits of \cite[Theorem 11]{amit2022integral}.
We exploit the supermartingale toolbox, recently introduced in PAC-Bayes framework by \cite{haddouche2023pac,chugg2023unified,jang2023tight}, to derive bounds for both batch learning (\Cref{theorem:supervised-ht,theorem:supervised-nnl}) and online learning (\Cref{theorem:online-ht,theorem:online}).


\subsection{PAC-Bayes for batch learning with \iid data}
\label{sec:wasserstein-batch}

In this section, we use the batch setting described in \Cref{sec:framework}.
We state our first result, holding for heavy-tailed losses admitting order 2 moments.
Such an assumption is in line, for instance, with reinforcement learning with heavy-tailed reward (see, \eg, \citealp{liu2011multi,lu2019optimal,zhuang2021regret}).

\begin{restatable}{theorem}{theoremsupervisedht}\label{theorem:supervised-ht}
We assume the loss $\loss$ to be $L$-Lipschitz.
Then, for any $\delta\in(0,1]$, for any sequence of positive scalar $(\lambda_i)_{i\in \{1,\dots,K\}}$, with probability at least $1-\delta$ over the sample $\S$, the following holds for the distributions $\P_{i,\S}\defeq \P_i(\S,.)$ and for any $\Q\in\Mcal(\H)$: 
\begin{multline*}
        \EE_{h\sim\Q}\Big[ \Risk_{\D}(h) - \Riskhat_{\Sm}(h) \Big]  \\ \leq    \sum_{i=1}^{K} \frac{2|\S_i|L}{m}\W(\Q, \P_{i,\S}) + \frac{1}{m}\sum_{i=1}^{K}   \frac{\ln\left( \frac{K}{\delta}  \right)}{\lambda_i} + \frac{\lambda_i}{2}\left(\EE_{h\sim \P_{i,\S}}\LB\Vhat_{|\S_i|}(h) + V_{|\S_i|}(h) \RB\right), 
    \end{multline*}
where $\P_{i,\S}$ {\it does not} depend on $\S_i$. 
Also, for any $i,|S_i|$, we have $\Vhat_{|\S_i|}(h)= \sum_{\z\in \S_i} \left(\loss(h,\z) - R_\D(h)\right)^2$ and $V_{|\S_i|}(h) = \EE_{\S_i}\LB\Vhat_{|\S_i|}(h)\RB$.
\end{restatable}

The proof is deferred to \Cref{sec:proof-supervised-ht}.
While \Cref{theorem:supervised-ht} holds for losses taking values in $\R$, many learning problems rely in practice on more constrained losses.
This loss can be bounded as in the case of, \eg, supervised learning or the multi-armed bandit problem \citep{slivkins2019intro}, or simply non-negative as in regression problems involving the quadratic loss (studied, for instance, in \citealp{catoni2016pac,catoni2017dimension}).
Using again the supermartingale toolbox, we prove in \Cref{theorem:supervised-nnl} a tighter bound holding for heavy-tailed non-negative losses. 

\begin{restatable}{theorem}{theoremsupervisednnl}
\label{theorem:supervised-nnl}
We assume our loss $\loss$ to be non-negative and $L$-Lipschitz. We also assume that, for any $1\leq i\leq K$, for any dataset$\S$, we have $\EE_{h\sim \P_{i}(.,\S), z\sim \D}\LB \loss(h,z)^2 \RB \leq 1$ (\emph{bounded order 2 moments for priors}).
Then, for any $\delta\in(0,1]$, with probability at least $1-\delta$ over the sample $\S$, the following holds for the distributions $\P_{i,\S}\defeq \P_i(\S,.)$ and for any $\Q\in\Mcal(\H)$: 
\begin{align*}
\ \EE_{h\sim\Q}\Big[ \Risk_{\D}(h) - \Riskhat_{\Sm}(h) \Big] \le \sum_{i=1}^{K} \frac{2|\S_i|L}{m} \W(\Q, \P_{i,\S}) + \sum_{i=1}^{K} \sqrt{\frac{2|\S_i|\ln\frac{K}{\delta}}{m^2}}, 
\end{align*}
where $\P_{i,\S}$ {\it does not} depend on $\S_i$.
\end{restatable}

Note that when the loss function takes values in $[0,1]$, an alternative strategy allows tightening the last term of the bound by a factor $\frac{1}{2}$.
This result is rigorously stated in \Cref{theorem:supervised_tight} of \Cref{sec:alt-proof-supervised}. 

\textbf{High-level ideas of the proofs.}
\Cref{theorem:supervised-ht,theorem:supervised-nnl} are structured around two tools.
First, we exploit the Kantorovich-Rubinstein duality \cite[Remark 6.5]{villani2009optimal} to replace the change of measure inequality \cite{csizar1975divergence,donsker1976asymp}; this allows us to consider a Wasserstein distance instead of a KL term.
Then, we exploit the supermartingales used in \cite{haddouche2023pac,chugg2023unified} alongside Ville's inequality (instead of Markov's one) to obtain a high probability bound holding for heavy-tailed losses.
Combining those techniques provides our PAC-Bayesian bounds.

\textbf{Analysis of our bounds.} Our results hold for Lipschitz losses and allow us to consider heavy-tailed losses with bounded order 2 moments.
While such an assumption on the loss is more restrictive than in classical PAC-Bayes, allowing heavy-tailed losses is strictly less restrictive. 
While \Cref{theorem:supervised-ht} is our most general statement, \Cref{theorem:supervised-nnl} allows recovering a tighter result (without empirical variance terms) for non-negative heavy-tailed losses. 
An important point is that the variance terms are considered with respect to the prior distributions $\P_{i,\S}$  and not $\Q$ as in \cite{haddouche2023pac,chugg2023unified}. This is crucial as these papers rely on the implicit assumption of order 2 moments, holding uniformly for all $\Q\in\Mcal(\H)$, while we only require this assumption for the prior distributions $(\P_{i,\S})_{i=1,\dots,K}$.
Such an assumption is in line with the PAC-Bayesian literature, which often relies on bounding an averaged quantity with respect to the prior.
This strength is a consequence of the Kantorovich-Rubinstein duality.
To illustrate this, consider \iid data with distribution $\D$ admitting a finite variance bounded by $V$ and the loss $\loss(h,z)= |h-z|$ where both $h$ and $z$ lie in the real axis.
Notice that in this particular case, we can imagine that $z$ is a data point and $h$ is a hypothesis outputting the same scalar for all data.
To satisfy the assumption of \Cref{theorem:supervised-nnl}, it is enough, by Cauchy Schwarz, to satisfy  $\EE_{h\sim \P_{i,\S},z\sim\S}[\loss(h,z)^2] \le \EE[h^2] + 2V\EE[|h|] +V^2 \leq 1$ for all $\P_{i,\S}$.
On the contrary, \cite{haddouche2023pac,chugg2023unified} would require this condition to hold for all $\Q$, which is more restrictive.
Finally, an important point is that our bound allows us to consider Dirac distributions with disjoint support as priors and posteriors.
On the contrary, KL divergence forces us to consider a non-Dirac prior for our bound to be non-vacuous. 
This allows us to retrieve a uniform-convergence bound described in \Cref{corollary:supervised-nnl}.

\textbf{Role of data-dependent priors.} 
\Cref{theorem:supervised-ht,theorem:supervised-nnl} allow the use of prior distributions depending possibly on a fraction of data.
Such a dependency is crucial to control our sum of Wasserstein terms as we do not have an explicit convergence rate.
For instance, for a fixed $K$, consider a compact predictor space $\H$, a bounded loss and the \emph{Gibbs posterior} defined as $d\Q(h) \propto \exp\left(-\lambda \Riskhat_{\Sm}(h)\right)dh$ where $\lambda>0$.
Also define for any $i$ and $\S$, the distribution $d\P_{i,\S}(h) \propto \exp\left(-\lambda \Risk_{\S/\S_i}(h)\right)dh$. Then, by the law of large numbers, when $m$ goes to infinity, for any $h$, both $\Risk_S(h)$ and $(\Risk_{\S/\S_i}(h))_{i=1,\dots,m}$ converge to $\Risk_\D(h)$. 
This ensures, alongside with the dominated convergence theorem, that for any $i$, the Wasserstein distance $\W(\Q,\P_{i,\S})$ goes to zero as $m$ goes to infinity.  

\textbf{Comparison with the literature.} \cite[][Theorem 11]{amit2022integral} establishes a PAC-Bayes bound with Wasserstein distance valid for bounded losses being Lipschitz with high probability. While we circumvent the first assumption, the second one is less restrictive than actual Lipschitzness and can also be used in our setting. Also \cite[Theorem 12]{amit2022integral} proposes an explicit convergence for finite predictor classes. We show in \Cref{sec:discussion-supervised} that we are also able to recover such a convergence. 

\textbf{Towards new PAC-Bayesian algorithms.} From \Cref{theorem:supervised-nnl}, we derive a new PAC-Bayesian algorithm for Lipschitz non-negative losses:
\begin{equation}
    \label{eq:batch-alg}
    \argmin_{\Q\in\Mcal(\H)} \EE_{h\sim \Q}\left[\Riskhat_{\Sm}(h)\right] + \sum_{i=1}^{K} \frac{2|\S_i|L}{m} \W(\Q, \P_{i,\S}).
\end{equation}
\Cref{eq:batch-alg} uses Wasserstein distances as regularisers and allows the use of multiple priors. 
We compare ourselves to the classical PAC-Bayes algorithm derived from \cite[][Theorem 1.2.6]{catoni2007pac} (which leads to Gibbs posteriors):
\begin{equation}
    \label{eq:catoni-alg}
    \argmin_{\Q\in\Mcal(\H)} \EE_{h\sim \Q}\left[\Riskhat_{\Sm}(h)\right] +  \frac{\operatorname{KL}(\Q,\P)}{\lambda}.
\end{equation}
Considering a Wasserstein distance in \Cref{eq:batch-alg} makes our algorithm more flexible than in \Cref{eq:catoni-alg}, the KL divergence implies absolute continuity \wrt the prior $\P$.
Such an assumption is not required to use \Cref{eq:batch-alg} and covers the case of prior Dirac distributions.
Finally, \Cref{eq:batch-alg} relies on a fixed value $K$ whose value is discussed below.

\textbf{Role of $K$.} 
We study the cases $K=1$, $\sqrt{m}$, and $m$ in \Cref{theorem:supervised-nnl}. We refer to \Cref{sec:discussion-supervised} for a detailed treatment.
First of all, when $K=1$, we recover a classical batch learning setting where all data are collected at once.
In this case, we have a single Wasserstein with no convergence rate coupled with a statistical ersatz of $\sqrt{\frac{\ln(1/\delta)}{m}}$.
However, similarly to \cite[][Theorem 12]{amit2022integral}, in the case of a finite predictor class, we are able to recover an explicit convergence rate.
The case $K=\sqrt{m}$ provides a tradeoff between the number of points required to have good data-dependent priors (which may lead to a small $\sum_{i=1}^{\sqrt{m}}\W(\Q, \P_i)$) and the number of sets required to have an explicit convergence rate. 
Finally, the case $K=m$ leads to a vacuous bound as we have the incompressible term $\sqrt{\ln\left(\frac{m}{\delta}\right)}$, which makes the bound vacuous for large values of $m$.
This means that the batch setting is not fitted to deal with a data stream arriving sequentially. To mitigate that weakness, we propose in \Cref{sec:wasserstein-online} the first online PAC-Bayes bounds with Wasserstein distances.

\subsection{Wasserstein-based generalisation bounds for online learning}
\label{sec:wasserstein-online}
Here, we use the online setting described in \Cref{sec:framework} and derive the first online PAC-Bayes bounds involving Wasserstein distances in \Cref{theorem:online-ht,theorem:online}. 
Online PAC-Bayes bounds are meant to derive online counterparts of classical PAC-Bayesian algorithms \cite{haddouche2022online}, where the KL-divergence acts as a regulariser.
We show in  \Cref{theorem:online-ht,theorem:online} that it is possible to consider online PAC-Bayesian algorithms where the regulariser is a Wasserstein distance, which allows us to optimise on measure spaces without a restriction of absolute continuity.

\begin{restatable}{theorem}{theoremonlineht}\label{theorem:online-ht}
 We assume our loss $\loss$ to be $L$-Lipschitz.
Then, for any $\delta\in(0,1]$, with probability at least $1-\delta$ over the sample $\S$, the following holds for the distributions $\P_{i,\S}\defeq \P_i(\S,.)$ and for any sequence $(\Q_i)_{i=1\cdots m}\in\Mcal(\H)^m$:
\begin{multline*}
\sum_{i=1}^m \EE_{h_i\sim \Q_{i}} \Big[\EE[\loss(h_i,\z_i) \mid \Fcal_{i-1}] - \loss(h_i,\z_i) \Big]  \le 2L\sum_{i=1}^{m}\W(\Q_{i}, \P_{i,\S}) \\
+ \frac{\lambda}{2}\sum_{i=1}^m \EE_{h_i\sim \P_{i,\S}}\left[ \Vhat_i(h_i,\z_i) + V_i(h_i) \right]+\frac{\ln(1/\delta)}{\lambda}, 
\end{multline*}
where for all $i$, $\Vhat_i(h_i,\z_i)= (\loss(h_i,\z_i)-\EE_{i-1}[\loss(h_i,\z_i)])^2$ is the conditional empirical variance at time $i$ and $V_i(h_i)= \EE_{i-1}[\Vhat(h_i,\z_i)]$ is the true conditional variance.
\end{restatable}

The proof is deferred to \Cref{sec:proof-online-ht}.
We also provide the following bound, being an online analogous of \Cref{theorem:supervised-nnl}, valid for non-negative heavy-tailed losses.


\begin{restatable}{theorem}{theoremonline}
\label{theorem:online}
We assume our loss $\loss$ to be non-negative and $L$-Lipschitz.
We also assume that, for any $i,\S$, $\EE_{h\sim \P_{i}(.,\S)}\LB \EE_{i-1}[\loss(h,\z_i)^2] \RB \leq 1$ (\emph{bounded conditional order 2 moments for priors}).
Then, for any $\delta\in(0,1]$, with probability at least $1-\delta$ over the sample $\S$, any online predictive sequence (used as priors) $(\P_i)_{i\geq 1}$, we have with probability at least $1-\delta$ over the sample $S\sim\D$, the following, holding for the data-dependent measures $\P_{i,\S}\defeq \P_i(S,.)$ and any posterior sequence $(\Q_i)_{i\geq 1}$:
\begin{align*}
\frac{1}{m}\sum_{i=1}^m \EE_{h_i\sim \Q_{i}} \Big[\EE[\loss(h_i,\z_i) \mid \Fcal_{i-1}] - \loss(h_i,\z_i) \Big]  \le \frac{2L}{m}\sum_{i=1}^{m}\W(\Q_{i}, \P_{i,\S}) + \sqrt{\frac{2\ln\LP\frac{1}{\delta}\RP}{m}}.
\end{align*}
\end{restatable}
The proof is deferred to \Cref{sec:proof-online}.

\textbf{Analysis of our bounds.} 
\Cref{theorem:online-ht,theorem:online} are, to our knowledge, the first results involving Wasserstein distances for online PAC-Bayes learning.
They are the online counterpart of \Cref{theorem:supervised-ht,theorem:supervised-nnl}, and the discussion of \Cref{sec:wasserstein-batch} about the involved assumptions also apply here.
The sum of Wasserstein distances involved here is a consequence of the online setting and must grow sublinearly for the bound to be tight.
For instance, when $(\Q_i=\delta_{h_i})_{i\geq1}$ is the output of an online algorithm outputting Dirac measures and $\P_{i,\S}= \Q_{i-1}$, the sum of Wasserstein is exactly $\sum_{i=1}^m d(h_i,h_{i-1})$.
This sum has to be sublinear for the bound to be non-vacuous, and the tightness depends on the considered learning problem. 
An analogous of this sum can be found in dynamic online learning \cite{zinkevich2003online} where similar sums appear as \emph{path lengths} to evaluate the complexity of the problem.  

\textbf{Comparison with literature.}
We compare our results to existing PAC-Bayes bounds for martingales of \cite{seldin2012pac}. 
\cite[Theorem 4]{seldin2012pac} is a PAC-Bayes bound for martingales, which controls an average of martingales, similar to our \Cref{theorem:supervised-ht}.
Under a boundedness assumption, they recover a McAllester-typed bound, while \Cref{theorem:supervised-ht} is more of a Catoni-typed result.
Also, \cite[Theorem 7]{seldin2012pac} is a Catoni-typed bound involving a conditional variance, similar to our \Cref{theorem:online}.
They require to bound uniformly the variance on all the predictor sets, while we only assume averaged variance with respect to priors, which is what we required to perform \Cref{theorem:online}.

\textbf{A new online algorithm.}
\cite{haddouche2022online} derived from their main theorem, an online counterpart of \Cref{eq:catoni-alg}, proving it comes with guarantees.
Similarly, we exploit \Cref{theorem:online} to derive the online counterpart of \Cref{eq:batch-alg}, from the data-free initialisation $\Q_1$

\begin{equation}
    \label{eq:online-alg}
    \forall i \geq 1,\ \ \Q_i \in \argmin_{\Q\in\Mcal(\H)} \EE_{h\sim \Q}\left[\loss(h_i, \z_i)\right] + 2L\W(\Q, \P_{i,\S}).
\end{equation}

We highlight the merits of the algorithm defined by \Cref{eq:online-alg}, alongside with the one from \Cref{eq:batch-alg}, in \Cref{sec:experiments}.

\section{Learning via Wasserstein regularisation}
\label{sec:experiments}

\Cref{theorem:supervised-nnl,theorem:online} are designed to be informative on the generalisation ability of a single hypothesis even when Dirac distributions are considered.
In particular, our results involve Wasserstein distances acting as regularisers on $\H$. 
In this section, we show that a Wasserstein regularisation of the learning objective, which comes from our theoretical bounds, helps to better generalise in practice.
Inspired by \Cref{eq:batch-alg,eq:online-alg}, we derive new PAC-Bayesian algorithms for both batch and online learning involving a Wasserstein distance (see \Cref{sec:algo}), we describe our experimental framework in \Cref{sec:exp-fram} and we present some of the results in \Cref{sec:results}.
Additional details, experiments, and discussions are gathered in \Cref{sec:supplementary-expes} due to space constraints. 
All the experiments are reproducible with the source code provided on GitHub at \url{https://github.com/paulviallard/NeurIPS23-PB-Wasserstein}.

\subsection{Learning algorithms}
\label{sec:algo}

\textbf{Classification.} 
In the classification setting, we assume that the data space $\Z=\X{\times}\Y$ is composed of a $d$-dimensional \textit{input space} $\X=\{ \x \in \R^d \;|\; \|\x\|_2 \le 1 \}$ and a finite \textit{label space} $\Y=\{1,\dots, |\Y|\}$ with $|\Y|$ labels.
We aim to learn models $h_{\wbf}: \R^d\to \R^{|\Y|}$ parameterised by a weight vector $\wbf$ that outputs, given an input $\x\in\X$, a score $h_{\wbf}(\x)[y'] \in \R$ for each label $y'$. 
This score allows us to assign a label to $\x\in\X$; to check if $h_{\wbf}$ classifies correctly the example $(\x, y)$, we use the {\it classification loss} defined by $\loss^{c}(h_{\wbf}, (\x, y)) \defeq \mathds{1}\left[ h_{\wbf}(\x)[y] - \max_{y'\ne y} h_{\wbf}(\x)[y'] \le 0 \right]$, where $\mathds{1}$ denotes the indicator function.

\textbf{Batch algorithm.} In the batch setting, we aim to learn a parametrised hypothesis $h_{\wbf}\in\H$ that minimises the population classification risk $\Rfrak_{\D}(h_{\wbf}) = \EE_{(\x, y)\sim\D}\loss^{c}(h_{\wbf}, (\x, y))$ that we can only estimate through the empirical classification risk $\Rfrak_{\S}(h_{\wbf}) = \frac{1}{m}\sum_{i=1}^{m}\loss^{c}(h_{\wbf}, (\x_i, y_i))$.
To learn the hypothesis, we start from \Cref{eq:batch-alg}, when the distributions $\Q$ and $\P_1,\dots, \P_K$ are Dirac masses, localised at $h_{\wbf}, h_{\wbf_1},\dots h_{\wbf_K}\in\H$ respectively.
Indeed, in this case, $\W(\Q, \P_{i,\S}) = d(h_{\wbf}, h_{\wbf_i})$ for any $i$.
However, the loss $\loss^{c}(.,\z)$ is not Lipschitz and the derivatives are zero for all examples $\z\in\X\times\Y$, which prevents its use in practice to obtain such a hypothesis $h_{\wbf}$.
Instead, for the population risk $\Risk_{\D}(h)$ and the empirical risk $\Riskhat_{\Sm}(h)$ (in \Cref{theorem:supervised-nnl} and \Cref{eq:batch-alg}), we consider the loss $\loss(h, (\x, y)) = \frac{1}{|\Y|}\sum_{y'\ne y} \max(0, 1{-} \eta(h[y]{-}h[y']))$, which is $\eta$-Lipschitz \wrt the outputs $h[1],\dots, h[|\Y|]$.
This loss has subgradients everywhere, which is convenient in practice.
We go a step further by {\it (a)}  setting $L=\frac{1}{2}$ and {\it (b)} adding a parameter $\varepsilon>0$ to obtain the objective
\begin{align}
    \label{eq:batch-alg-exp}
    \argmin_{h_{\wbf}\in\Hcal}\left\{ \Riskhat_{\Sm}(h_{\wbf}) + \varepsilon\LB\sum_{i=1}^{K} \frac{|\S_i|}{m} d\LP h_\wbf,h_{\wbf_i}\RP\RB\right\{.
\end{align}
To (approximately) solve \Cref{eq:batch-alg-exp}, we propose a two-step algorithm.
First, \textsc{Priors Learning} learns $K$ hypotheses $h_{\wbf_1}, \dots, h_{\wbf_K} \in \H$ by minimising the empirical risk via stochastic gradient descent.
Second, \textsc{Posterior Learning} learns the hypothesis $h_{\wbf}\in\H$ by minimising the objective associated with \Cref{eq:batch-alg-exp}.
More precisely, \textsc{Priors Learning} outputs the hypotheses $h_{\wbf_1},\cdots,h_{\wbf_K}$, obtained by minimising the empirical risk through mini-batches.
Those batches are designed such that for any $i$, the hypothesis $h_{\wbf_i}$ does not depend on $\S_i$.
Then, given $h_{\wbf_1}, \dots, h_{\wbf_K} \in \H$, \textsc{Posterior Learning} minimises the objective in \Cref{eq:batch-alg-exp} with mini-batches.
Those algorithms are presented in \Cref{alg:batch} of \Cref{sec:supplementary-expes}.
While $\varepsilon$ is not suggested by \Cref{eq:batch-alg}, it helps to control the impact of the regularisation in practice.
\Cref{eq:batch-alg-exp} then optimises a tradeoff between the empirical risk and the regularisation term $\varepsilon\sum_{i=1}^{K} \frac{|\S_i|}{m} d(h_{\wbf}, h_{\wbf_i})$.

\textbf{Online algorithm.} Online algorithms output, at each time step $i \in \{1, \dots, m\}$, a new hypothesis $h_{\wbf_i}$. 
From \Cref{eq:online-alg}, particularised to a sequence of Dirac distributions (localised in $h_{\wbf_1},\cdots,h_{\wbf_K})$, we design a novel online PAC-Bayesian algorithm with a Wasserstein regulariser:
\begin{align}
    \label{eq:online-alg-exp}
    \forall i \geq 1,\ \ h_i\in \argmin_{h_{\wbf}\in \Hcal} \loss(h_{\wbf}, \z_i) +d\LP h_{\wbf},h_{\wbf_{i-1}}\RP \ \ \text{\emph{s.t.}} \ \ d\LP h_{\wbf},h_{\wbf_{i-1}}\RP \le 1.
\end{align}
According to \Cref{theorem:online}, such an algorithm aims to bound the {\it population cumulative classification loss} $\mathfrak{C}_{\D} = \sum_{i=1}^{m}\EE[\loss^{c}(h_{\wbf_i},\z_i) \mid \Fcal_{i-1}]$.
Note that we added the constraint $d\LP h_{\wbf},h_{\wbf_{i-1}}\RP \le 1$ compared to \Cref{eq:online-alg}. 
This constraint ensures that the new hypothesis $h_{\wbf_i}$ is not too far from $h_{\wbf_{i-1}}$ (in the sense of the distance $\|\cdot\|_2$).
Note that the constrained optimisation problem in \Cref{eq:online-alg-exp} can be rewritten in an unconstrained form (see \cite{boyd2004convex}) thanks to a barrier $B(\cdot)$ defined by $B(a)=0$ if $a\le 0$ and $B(a)=+\infty$ otherwise; we have 
\begin{align} 
    \label{eq:online-alg-exp-2}
    \forall i \geq 1,\ \ h_i\in \argmin_{h_{\wbf}\in \Hcal} \loss(h_{\wbf}, \z_i) + d\LP h_{\wbf},h_{\wbf_{i-1}}\RP+ B(d\LP h_{\wbf},h_{\wbf_{i-1}}\RP-1).
\end{align}
When solving the problem in \Cref{eq:online-alg-exp-2} is not feasible, we approximate it with a log barrier of \cite{kervadec2022constrained} (suitable in a stochastic gradient setting); given a parameter $t>0$, the log barrier extension is defined by $\Bhat(a) = -\frac{1}{t}\ln(-a)$ if $a\le -\frac{1}{t^2}$ and $\Bhat(a) = ta-\frac{1}{t}\ln(\frac{1}{t^2})+\frac{1}{t}$ otherwise.
We present in \Cref{sec:supplementary-expes} \Cref{alg:online} that aims to (approximately) solve \Cref{eq:online-alg-exp-2}.
To do so, for each new example $(\x_i, y_i)$, the algorithm runs several gradient descent steps to optimise \Cref{eq:online-alg-exp-2}.


\subsection{Experimental framework}
\label{sec:exp-fram}

In this part, we assimilate the predictor space $\H$ to the parameter space $\R^d$.
Thus, the distance $d$ is the Euclidean distance between two parameters: $d\LP h_{\wbf},h_{\wbf'}\RP = \|\wbf{-}\wbf'\|_2$.
This implies that the Lipschitzness of $\loss$ has to be taken \wrt $\wbf$ instead of $h_{\wbf}$.

\textbf{Models.} 
We consider that the models are either linear or neural networks (NN).
Linear models are defined by $h_{\wbf}(\x)=W\x+b$, where $W\in\R^{|\Y|\times d}$ is the weight matrix, $b\in\R^{|\Y|}$ is the bias, and $\wbf=\vect(\{W, b\})$ its vectorisation; the vector $\wbf$ with the zero vector.
Thanks to the definition of $\X$, we know from \Cref{lemma:lipschitz-linear} (and the composition of Lipschitz functions) that the loss is $\sqrt{2}\eta$-Lipschitz \wrt $\wbf$.
For neural networks, we consider fully connected ReLU neural networks with $L$ hidden layers and $D$ nodes, where the leaky ReLU activation function $\ReLU: \R^D\to\R^D$ applies elementwise $x\mapsto \max(x, 0.01x)$.
More precisely, the network is defined by $h_{\wbf}(\x) = Wh^{L}(\cdots h^{1}(\x))+b$ where $W\in \R^{|\Y|\times D}$, $b\in\R^{|\Y|}$. Each layer $h^{i}(\x)=\ReLU(W_i\x+b_i)$ has a weight matrix $W_i\in \R^{D\times D}$ and bias $b_i\in \R^{D}$ except for $i=1$ where we have $W_1\in \R^{D\times d}$. 
The weights $\wbf$ are also the vectorisation $\wbf=\vect(\{W, W_{L}, \dots, W_1, b, b_{L}, \dots, b_1\})$. 
We have precised in \Cref{l:lip-nn} that our loss is Lipschitz \wrt the weights $\wbf$.
We initialise the network similarly to \cite{dziugaite2017computing} by sampling the weights from a Gaussian distribution with zero mean and a standard deviation of $\sigma=0.04$; the weights are further clipped between $-2\sigma$ and $+2\sigma$.
Moreover, the values in the biases $b_1,\dots, b_L$ are set to 0.1, while the values for $b$ are set to $0$.
In the following, we consider $D=600$ and $L=2$; more experiments are considered in the appendix.

\textbf{Optimisation.} 
To perform the gradient steps, we use the COCOB-Backprop optimiser~\cite{orabona2017training} (with parameter $\alpha=10000$).\footnote{The parameter $\alpha$ in COCOB-Backprop can be seen as an initial learning rate; see \cite{orabona2017training}.}
This optimiser is flexible as the learning rate is adaptative and, thus, does not require hyperparameter tuning.
For \Cref{alg:batch}, which solves \Cref{eq:batch-alg-exp}, we fix a batch size of $100$, \ie, $|\mathcal{U}|=100$, and the number of epochs $T$ and $T'$ are fixed to perform at least $20000$ iterations.
Regarding \Cref{alg:online}, which solves \Cref{eq:online-alg-exp-2}, we set $t=100$ for the log barrier, which is enough to constrain the weights and the number of iterations to $T=10$.

\textbf{Datasets.} 
We study the performance of \Cref{alg:batch,alg:online} on UCI datasets~\citep{dua2017uci} along with MNIST~\citep{lecun1998mnist} and FashionMNIST~\citep{xiao2017fashion}.
We also split all the data (from the original training/test set) in two halves; the first part of the data serves in the algorithm (and is considered as a training set), while the second part is used to approximate the population risks $\Rfrak_{\D}(h)$ and $\mathfrak{C}_{\D}$ (and considered as a testing set).

\subsection{Results}
\label{sec:results}

We present in \Cref{tab:expe_1,tab:expe_2} the performance of \Cref{alg:batch,alg:online} compared to the Empirical Risk Minimisation (ERM) and the Online Gradient Descent (OGD) with the COCOB-Backprop optimiser.
\Cref{tab:linear_batch,tab:nn_batch} present the results of \Cref{alg:batch} for the \iid setting on linear and neural networks respectively, while \Cref{tab:linear_online,tab:nn_online} present the results of \Cref{alg:online} for the online case.

\begin{table}[ht]
    \caption{%
Performance of \Cref{alg:batch,alg:online} compared respectively to ERM and OGD on different datasets on linear models.
For the \iid setting, we consider $\varepsilon=\frac{1}{m}$ and $\varepsilon=\frac{1}{\sqrt{m}}$ and with $K=0.2\sqrt{m}$. 
For each method, we plot the empirical risk $\Rfrak_{\S}(h)$ or $\mathfrak{C}_{\S}$ with its associated test risk $\Rfrak_{\D}(h)$ or $\mathfrak{C}_{\D}$.
The risk in {\bf bold} corresponds to the lowest one among the ones considered.
For the online case, the two population risks are \underline{underlined} when the absolute difference is lower than 0.05.
    }
   \begin{subtable}{0.25\textwidth}
       \centering
       \caption{Linear model -- batch learning}
       \input{chapter_6/tables/paper_linear_batch}
      \label{tab:linear_batch}
   \end{subtable}
   \hfill
   \begin{subtable}{0.28\textwidth}
       \centering
       \caption{Linear model -- online learning}
       \input{chapter_6/tables/paper_linear_online}
       \label{tab:linear_online}
   \end{subtable}
   \label{tab:expe_1}
\end{table}

\begin{table}[ht]
    \caption{%
Performance of \Cref{alg:batch,alg:online} compared respectively to ERM and OGD on different datasets on neural network models.
For the \iid setting, we consider $\varepsilon=\frac{1}{m}$ and $\varepsilon=\frac{1}{\sqrt{m}}$ and with $K=0.2\sqrt{m}$. 
For each method, we plot the empirical risk $\Rfrak_{\S}(h)$ or $\mathfrak{C}_{\S}$ with its associated test risk $\Rfrak_{\D}(h)$ or $\mathfrak{C}_{\D}$.
The risk in {\bf bold} corresponds to the lowest one among the ones considered.
For the online case, the two population risks are \underline{underlined} when the absolute difference is lower than 0.05.
    }
   \begin{subtable}{0.25\textwidth}
       \centering
       \caption{NN model -- batch learning}
       \input{chapter_6/tables/paper_nn_batch}
      \label{tab:nn_batch}
   \end{subtable}
   \hfill
   \hspace{0.5cm}\begin{subtable}{0.28\textwidth}
       \centering
       \caption{NN model -- online learning}
       \input{chapter_6/tables/paper_nn_online}
       \label{tab:nn_online}
    \end{subtable}
    \label{tab:expe_2}
\end{table}

\textbf{Analysis of the results.} In batch learning, we note that the regularisation term brings generalisation improvements compared to the empirical risk minimisation.
Indeed, our batch algorithm (\Cref{alg:batch}) has a lower population risk $\Rfrak_{\D}(h)$ on 11 datasets for the linear models and 9 datasets for the neural networks. In particular, notice that NNs obtained from \Cref{alg:batch} are more efficient than the ones obtained from ERM on \textsc{MNIST} and \textsc{FashionMNIST}, which are the more challenging datasets.
This suggests that the regularisation term helps to generalise well.
For the online case, the performance of the linear models obtained from our algorithm (\Cref{alg:online}) and by OGD are comparable: we have a tighter population classification risk $\Rfrak_{\D}(h)$ on $5$ datasets over $13$. 
However, notice that the risk difference is less than $0.05$ on $6$ datasets.
The advantage of \Cref{alg:online} is more pronounced for neural networks: we improve the performance in all datasets except \textsc{ adult} and \textsc{ sensorless}.
Hence, this confirms that optimising the regularised loss $\loss(h_{\wbf}, \z_i) + \|\wbf{-}\wbf_{i-1}\|$ brings a good advantage compared to the loss $\loss(h_{\wbf}, \z_i)$ only. A possible explanation would be that OGD suffers from underfitting (with a high empirical risk $\mathfrak{C}_{\D}$) while we are able to control overfitting through a regularisation term.
Indeed, only one gradient descent step is done for each new datum $(\x_i, y_i)$, which might not be sufficient to decrease the loss.  
Instead, our method solves the problem associated with \Cref{eq:online-alg-exp-2} and constrains the descent with the norm $\|\wbf-\wbf_{i-1}\|$.

\section{Conclusion and Perspectives}

We derived novel generalisation bounds based on the Wasserstein distance, both for batch and online learning, allowing for the use of deterministic hypotheses through PAC-Bayes.
We derived new learning algorithms which are inspired by the bounds, with remarkable empirical performance on a number of datasets: we hope our work can pave the way to promising future developments (both theoretical and practical) of generalisation bounds based on the Wasserstein distance.
Given the mostly theoretical nature of our work, we do not foresee an immediate negative societal impact, although we hope a better theoretical understanding of generalisation will ultimately benefit practitioners of machine learning algorithms and encourage virtuous initiatives.