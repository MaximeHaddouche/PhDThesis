\chapter[Wasserstein PAC-Bayes Learning: Exploiting Optimisation Guarantees to Explain Generalisation]{Wasserstein PAC-Bayes Learning: Exploiting Optimisation Guarantees to Explain Generalisation}
\label{chap:dis-mu}
\addchapterlof
\addchapterloa
\addchapterloe

\vspace{-1.6cm}
\begin{center}
\textbf{This chapter is based on the following papers}\\
\end{center}
\vspace{-0.3cm}
\printpublication{haddouche2023wasserstein}
\\
\printpublication{viallard2023learning}

\vspace{-0.3cm}
\minitoc

\vspace{-0.2cm}

\begin{abstract}
    Put WPB here, precise that, when the prior is seen as the learning goal, it is possible for a certain optimisation algorithm to directly incorporate sound geometric optimisation guarantee into a generalisation bound, trading the hope to reach a flat minima with a sound convergence guarantees. However, this comes at the cost of the explicit impact of the dimension. Also put the paper with Paul(batch bounds) as a supplementary content.
\end{abstract}

\newpage

\section{Introduction}

