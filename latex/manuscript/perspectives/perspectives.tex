\chapter*{Conclusion and Perspectives}
\addcontentsline{toc}{chapter}{Conclusion and Perspectives}
\label{chap:conclu}

\section*{Conclusion}
In this thesis we studied various interplays between PAC-Bayes learning and optimisation. Doing so, we challenged various prerequisites of PAC-Bayes bounds:

\begin{itemize}
    \item \emph{Strong statistical assumptions.} The main conclusion of \Cref{chap: pb-ht} is that, in order to perform PAC-Bayes, no assumption stronger than finite variance is required. In particular, classical bounded or subgaussian assumptions on the loss can be replaced by bounded variance at no additional cost. Note also that in \Cref{chap: wass-pb}, it is even possible to relax this assumption to boundedness over a compact alongisde lipschitz or gradient lipschitz assumption. This is consistent with  optimisation which often involves such geometric assumptions. Furthermore, the supermartingale toolbox allows bounds holding for all dataset size simultaneously, which is consistent with, \eg, online optimisation.
    \item \emph{The information-theoretic perspective of the prior.} A major contribution has been to formalise perspectives on the prior differing from the Bayesian view paradigm. Indeed, by considering the prior either as an initialisation point or a learning objective, we derived novel PAC-Bayesian bounds aiming to either reduce the impact of the prior (\Cref{chap:online-pb,chap:gen-flat-minima}) when seen as initialisation or highlight it (\Cref{chap: wass-pb}) when seen as a learning objective. This drove the emergence of Online PAC-Bayes learning and the introduction of gradient norm or convergence guarantees in PAC-Bayes.
    \item \emph{ PAC-Bayes is useful for stochastic predictors only.}. Following the spirit of \citet{amit2022integral}, we developed Wasserstein PAC-Bayes to incorporate deterministic predictors within PAC-Bayes bounds, as such predictors are often involved in optimisation algorithms. To obtain explicit convergence rates with such bounds we exploited duality results from optimal transport in \Cref{chap: wass-pb}. We also incorporated directly convergence guarantees of the Bures-Wasserstein SGD in a generalisation bound, at the price of an explicit impact of the dimension of the predictor space. It is then hard to tackle the case of deep neural networks, this is why we developed in \Cref{chap: wpb-practical}, another kind of Wasserstein PAC-Bayes bounds, with no explicit convergence rate, but yielding learning algorithm exploitable for neural nets.
\end{itemize}

\section*{Perspectives}

This thesis left unanswered many important questions on the interplays of optimisation and generalisation. 

\begin{itemize}
    \item \emph{Can we relax the finite variance assumption to obtain generalisation bounds?} As proven \Cref{chap: pb-ht} and \citet{chugg2023unified} it is possible to extend a large body of generalisation bounds to the case of finite variance. An interesting question is whether such an assumption is relaxed, this would be of interest, for instance, to understand the case of heavy-tailed SGD \citep{gurbuzbalaban2020heavy} which may be modelled by LÃ©vy processes, often having infinite variance.
    \item \emph{Can we further exploit flat minima to understand generalisation?} \Cref{chap:gen-flat-minima} proposed the first PAC-Bayes generalisation bounds exploiting flat minima. However the \texttt{QSB} assumption is required to exploit those results. While we saw that such a condition is verified by small networks, whether this condition is verified for deep neural nets remains an open question. Furthermore, the only empirical bound we have implies gradient lipschitz loss, a condition possibly hard to reach for deep nets. Empirical evaluation of those results is then an interesting future lead.
    \item \emph{Can we reach Wasserstein PAC-Bayes bound as simple and efficient than a KL one?} As shown in \Cref{chap: wass-pb} and \Cref{chap: wpb-practical} we did not obtain Wasserstein PAC-Bayes bounds with explicit convergence rate and without the explicit impact of the dimension as in KL-based PAC-Bayes bounds. An open question is whether it is possible to obtain a Wasserstein PAC-Bayes with both these desirable properties simultaneously. 
    \item \emph{Investigating the links between Online Learning and PAC-Bayes}. \Cref{chap:online-pb} draws a link from PAC-Bayes toward online learning by deriving novel learning algorithms from Online PAC-Bayes bounds. Recently, the elegant work of \citet{lugosi2023onlinetopac} has taken the opposite perspective:  starting from an online game they retrieve various generalisation bound, including KL-based and Wasserstein-based ones. Given the direct connection between online learning and the supermartingale framework \citep{wintenberger2021stochastic}, obtaining a unifying framework encompassing Wasserstein and KL-based PAC-Bayes from online learning for heavy-tailed losses is a promising future lead. A first step in this direction has recently been made \citet{viallard2024tighter} but does not involve online learning and holds only for bounded losses. 
\end{itemize}

Investigating those leads, and then reaching a better understanding of the impact of optimisation on generalisation are exciting questions for future work\pokeball