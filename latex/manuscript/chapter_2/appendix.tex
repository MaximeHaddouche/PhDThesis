%!TEX root = main.tex
\chapter{Appendix of Chapter~\ref{chap: pb-ht}}
\label{ap: pb-ht}

\begin{noaddcontents}
    

\section{Some PAC-Bayesian background}
\label{sec: pac_b_background}

We present below an immediate corollary of \citet[Thm 2.1]{seldin2012bandit} where we upper bounded the cumulative by an empirical quantity (the sum of squared upper bound of the martingale difference sequence).

\begin{theorem}[\citealp{seldin2012bandit}, Theorem 2.1]
\label{th: seldin_thm_mart}
Let $\left\{C_1, C_2, \ldots\right\}$ be an increasing sequence set in advance, such that $\left|X_i(\\S_i,h)\right| \leq C_i$ for all $\\S_i,h$ with probability 1.   Let $\left\{P_1, P_2, \ldots\right\}$ be a sequence of data-free prior distributions over $\mathcal{H}$. Let $(\lambda_i)_{i\geq 1}$ be a sequence of positive numbers such that
$$
\lambda_m \leq \frac{1}{C_m}.
$$
Then with probability $1-\delta$ over $\S=(\z_i)_{i\geq 1}$,
for all $m\geq 1$, any posterior $\Q$ over $\mathcal{H}$,
$$
\left|M_m\left(Q\right)\right| \leq \frac{\KL\left(\Q , \P_m\right)+2 \log (m+1)+\log \frac{2}{\delta}}{\lambda_m}+(e-2) \lambda_m V_m(\Q),
$$
where $V_m(\Q)$ is defined in \cref{subsec: comparison_seldin}.

Furthermore, if we bound the variance term, we would have:
$$
\left|M_m\left(\Q\right)\right| \leq \frac{\KL\left(\Q , \P_m\right)+2 \log (m+1)+\log \frac{2}{\delta}}{\lambda_m}+(e-2) \lambda_m \sum_{i=1}^m C_i^2.
$$
\end{theorem}
Below, we use the definitions introduced in \Cref{sec: iid_case}.
We study here a particular case of \cite{alquier2016properties} for bounded losses which are especially subgaussian thanks to Hoeffding's lemma.
\begin{theorem}[Adapted from  \citealp{alquier2016properties}, Theorem 4.1]
\label{th: naive_pac_bayes-chap3}
Let $m>0$,$\S_m=(\z_1,...,\z_m)$ be an \iid sample from the same law $\mu$.
For any data-free prior $\P$, for any loss function $\ell$ bounded by $K$, any $\lambda>0,\delta\in ]0;1[$, one has with probability $1-\delta$ for any posterior $Q\in\mathcal{M}_1(\mathcal{H})$
\[ \mathbb{E}_{h\sim \Q}[\Risk(h)] \leq  \mathbb{E}_{h\sim \Q}[\Riskhat_{\Sm}(h)] + \frac{\operatorname{KL}(\Q, \P) + \log(1/\delta)}{\lambda} + \frac{\lambda K^2}{2m}. \]
\end{theorem}

\begin{theorem}[\citealp{haddouche2021pac}, Theorem 3]
\label{th: haddouche_thm}
Let the loss $\ell$ be $\mathrm{HYPE}(K)$ compliant. For any $\P\in\mathcal{M}(\mathcal{H})$ with no data dependency, for any $\alpha\in\mathbb{R}$ and for any $\delta\in[0,1]$, we have with probability at least $1-\delta$ over size-$m$ samples S,
for any $\Q$% such that $Q \ll P$% and $P \ll Q$
\begin{align*}
\mathbb{E}_{h\sim \Q}\left[ \Risk(h)\right]
\leq \mathbb{E}_{h\sim \Q}\left[ \Riskhat_{\Sm}(h)\right] + \frac{\operatorname{KL}(\Q,\P) + \log\left(\frac{1}{\delta}\right)}{m^{\alpha}}
+\frac{1}{m^{\alpha}}\log\left(\mathbb{E}_{h\sim \P} \left[\exp\left( \frac{K(h)^2}{2m^{1-2\alpha}} \right) \right]\right).
\end{align*}
\end{theorem}


\section{Extensions of previous results}
\label{sec: extensions}

Here we gather several corollaries of our main result in order to show how our \Cref{th: main_thm} extends the validity of some classical results in the literature. More precisely we show that our result extends (up to numerical factors) the PAC-Bayes Bernstein inequality of \citet{seldin2012bandit}.
Then, going back to the bounded case, we generalise a result from \citet{catoni2007pac} reformulated in \citet{alquier2016properties} and we also show how our work strictly improves on the bound of \citet{haddouche2021pac}.

\subsection{Extension of the PAC-Bayes Bernstein inequality}
\label{subsec: comparison_seldin}

Here we rename two terms for consistency with Theorem 2.1 of \citet{seldin2012bandit} (see \Cref{th: seldin_thm_mart}). For a martingale $M_m(h)= \sum_{i=1}^m X_i(\S_i,h)$, we define, at time $m$, \emph{empirical cumulative variance } to be $\hat{V}_m(h)= [M]_m(h) = \sum_{i=1}^m X_i(\S_i,h)^2$ and
the \emph{cumulative variance} as $V_m(h)= \langle M\rangle_m(h) = \sum_{i=1}^m \mathbb{E}_{i-1}[X_i(\S_i,h)^2]$.

We provide below a corollary containing two bounds: the first one being a straightforward corollary of \cref{th: main_thm}, the second being valid for bounded martingales and formally close to Theorem 2.1 of \citet{seldin2012bandit}.
\begin{corollary}
\label{cor: bound_mart}
Let $\left\{\P_1, \P_2, \ldots\right\}$ be a sequence of data-free prior distributions over $\mathcal{H}$. Let $(\lambda_i)_{i\geq 1}$ be a sequence of positive numbers.
Then the following holds with probability $1-\delta$ over $\S=(\z_i)_{i\geq 1}$: for any tuple $(m,\lambda_k,\P_k)$ with $m,k\geq 1$, any posterior $\Q$ over $\mathcal{H}$,
\begin{align}
\label{eq: bound_mart_1}
\left|M_m\left(\Q\right)\right| \leq \frac{\KL\left(\Q, \P_k\right)+2 \log (k+1)+\log (2/\delta)}{\lambda_k}+ \frac{\lambda_k}{2}\left( \hat{V}_m(\Q) + V_m(\Q) \right),
\end{align}
with $\hat{V}_m(\Q)= \mathbb{E}_{h\sim \Q}[\hat{V}_m(h)], V_m(\Q)= \mathbb{E}_{h\sim \Q}[V_m(h)]$.
Furthermore, if we assume that for any $i$, there exists $C_i>0$ such that $|X_i(\S_i,h)|\leq C_i$ for all $\S_i,h$ then we have the following corollary: with probability $1-\delta$ over $S$, for any tuple $(m,\lambda_m,\P_m)$ $m\geq 1$, any posterior $\Q$,
\begin{align}
\label{eq: bound_mart_2}
\left|M_m\left(Q\right)\right| \leq \frac{\KL\left(\Q, \P_m\right)+2 \log (m+1)+\log (2/\delta)}{\lambda_m}+ \lambda_m\sum_{i=1}^m C_i^2.
\end{align}
\end{corollary}
The proof is deferred to \cref{sec: proofs}.
Note that \cref{eq: bound_mart_1} holds uniformly on all tuples $\{(\lambda_k,\P_k,m) \mid k\geq 1, m\geq 1\}$ while \cref{eq: bound_mart_2}, as well as Theorem 2.1 of \citet{seldin2012bandit} holds uniformly on the tuples
$\{(\lambda_m,\P_m,m) \mid m\geq 1\}$ which is a strictly smaller collection. Hence our approach gives guarantees for a larger event with the same confidence level.

Furthermore, Theorem 2.1 of \citet{seldin2012bandit} involves the cumulative variance $V_m(\Q)$ (and not its empirical counterpart). Because this term is theoretical, we bound it in \cref{th: seldin_thm_mart} by $\sum_{i=1}^m C_i^2$ which is supposedly empirical.
In this context, \cref{eq: bound_mart_2}, recovers nearly exactly the bound of \cite{seldin2012bandit} with the transformation of a factor $(e-2)$ into $1$.
Notice also that \cref{eq: bound_mart_2} stands with no assumption on the range of the $\lambda_i$, which is not the case in \cref{th: seldin_thm_mart}.

Finally, we stress two fundamental differences between our work and the one of \citet{seldin2012bandit}. First, we replace Markov's inequality by Ville's inequality; second, we exploited the exponential inequality of Lemma \ref{l: bercu_touati} instead of the Bernstein inequality. These allow for results for unbounded martingales for all $m$ simultaneously.


\subsection{Extensions of learning theory results}


\subsubsection{A general result for bounded losses}


We use definitions from \Cref{sec: iid_case} and provide a corollary of our main result when the loss is bounded by a positive constant $K>0$. We assume our data are iid.
\begin{corollary}
\label{cor: bounded_case}
For any data-free prior $P\in \mathcal{M}(\mathcal{H})$, any $\lambda>0$ the following holds with probability $1-\delta$ over the sample $S=(z_i)_{i\in\mathbb{N}}$, for all $m\in\mathbb{N}/\{0\}$, $\Q\in\mathcal{M}(\mathcal{H})$
\[ \left | \mathbb{E}_{h\sim \Q} [\Risk(h)] -  \mathbb{E}_{h\sim \Q} \left[\Riskhat_{\S_m}(h) \right] \right |  \leq \frac{\operatorname{KL}(\Q,\P) +\log(2/\delta)}{\lambda m } + \lambda K^2.  \]
We also have the local bound: for any $m\geq 1$, with probability $1-\delta$ over $S$, for all $\Q\in\mathcal{M}(\mathcal{H})$
\[ \mathbb{E}_{h\sim \Q} [\Risk(h)] \leq  \mathbb{E}_{h\sim \Q} \left[\Riskhat_{\S_m}(h) \right] + \frac{\operatorname{KL}(\Q,\P) +\log(2/\delta)}{\lambda} + \frac{\lambda K^2}{m}.  \]
\end{corollary}
The proof is deferred to \cref{sec: proofs}. Remark that the second bound of Corollary \ref{cor: bounded_case} is exactly the Catoni bound stated in \citet{alquier2016properties} (see \Cref{th: naive_pac_bayes-chap3} in \Cref{sec: pac_b_background}) up to a numerical factor of $2$.


The first bound is, to our knowledge, the first PAC-Bayesian bound for bounded losses holding uniformly (for a given parameter $\lambda$) on the choice of $Q,m$ and thus extends the scope of Catoni's bound which holds for a single $m$ with high probability.  Indeed, if we want for instance \Cref{th: naive_pac_bayes-chap3} to hold for any $i\in\{1..m\}$, we then have to take an union bound on $m$ events which turns the term $\log(1/\delta)$ into $\log(m/\delta)$ (but with the benefit of holding for $m$ parameters $\lambda_1,...,\lambda_m$). This point is common to the most classical PAC-Bayesian bounds
(including McAllester and Catoni's ones \eqref{eq: mcallester}, \eqref{eq: catoni})
and impeach us to have a bound uniformly on all $m\in\mathbb{N}/\{0\}$ as $\log(m)$ goes to infinity asymptotically.


\subsubsection{An extension of \citet{haddouche2021pac}}

We now focus on the work of \citet{haddouche2021pac} which provides general PAC-Bayesian bounds for unbounded losses. Their theorems hold for iid data and under the so-called \emph{HYPE} (for HYPothesis-dependent rangE) condition. It states that a loss function $\ell$ is \emph{HYPE}$(K)$ compliant if there exists a function $K:\mathcal{H} \rightarrow \mathbb{R}^+ $ (supposedly accessible)  such that $\forall z\in\mathcal{Z}, \ell(h,\z) \leq K(h)$.
We provide \Cref{cor: haddouche_comparison} to compare ourselves with their main result (stated in  \Cref{th: haddouche_thm} for convenience).
\begin{corollary}
\label{cor: haddouche_comparison}
For any data-free prior $\P\in \mathcal{M}(\mathcal{H})$, any loss function $\ell$ being \emph{HYPE}$(K)$ compliant, any $\alpha\in[0,1],m\geq 1$, the following holds with probability $1-\delta$ over the sample $\S=(\z_i)_{i\in\mathbb{N}}$, for all $\Q\in\mathcal{M}(\mathcal{H})$
\begin{multline*}
\mathbb{E}_{h\sim \Q} [\Risk(h)] \leq   \mathbb{E}_{h\sim \Q} \left[\frac{1}{m}\sum_{i=1}^m\left(\ell(h,\z_i) + \frac{1}{2m^{1-\alpha}} \ell(h,\z_i)^2\right)\right] \\
+ \frac{\operatorname{KL}(\Q,\P) +\log(1/\delta)}{m^{\alpha}}  + \frac{1}{2m^{1-\alpha}}\mathbb{E}_{h\sim \Q} [K^2(h)].
\end{multline*}
\end{corollary}

\begin{proof}
The proof is a straightforward application of \cref{th: main_thm_iid} by fixing $m\geq 1$ choosing $\lambda= m^{\alpha-1}$ (thus we localise \Cref{th: main_thm_iid} to a single $m$),  and bounding $\mathrm{Quad}(h)$ by $K^2(h)$.
\end{proof}
The main improvement of our bound over \Cref{th: haddouche_thm} is that we do not have to assume the convergence of an exponential moment to obtain a non-trivial bound. Indeed, we transformed the (implicit) assumption $\mathbb{E}_{h\sim \P} \left[\exp\left( \frac{K(h)^2}{2m^{1-2\alpha}} \right) \right] < +\infty $ onto $\mathbb{E}_{h\sim \Q}[K(h)^2] < +\infty$, which is significantly less restrictive.
Furthermore, \Cref{th: haddouche_thm} holds for a single choice of $m$ while ours still holds uniformly over all integers $m>0$.

Cor. \ref{cor: haddouche_comparison} also sheds new light on the \emph{HYPE} condition. Indeed, in \citet{haddouche2021pac}, $K$ only intervenes in an exponential moment involving the prior $\P$, while ours considers a second-order moment on $K$ implying the posterior $\Q$. The difference is major as $\mathbb{E}_{h\sim \Q}[K(h)^2] $ can be controlled by a wise choice of posterior. Thus it can be incorporated in our optimisation route, acting now as an optimisation constraint instead of an environment constraint.



\section{Proofs}
\label{sec: proofs}

\subsection{Proof of \cref{th: main_thm_iid}}

\begin{proof}
Let $\P$ a fixed data-free prior, set $(\mathcal{F}_i)_{i\geq 0}$ such that for all $i$, $\z_i$ is $\mathcal{F}_i$ measurable. We also set for any fixed $h\in\mathcal{H}, M_m(h):= \sum_{i=1}^m \ell(h,\z_i) - \Risk(h)$. Note that because data are \iid, for any fixed $h$, the sequence $(M_m(h))_m$ is indeed a martingale.
We set for any $m\geq 1, h\in\mathcal{H}$
$$[M]_m(h) = \sum_{i=1}^m \left(\ell(h,\z_i) - \Risk(h)\right)^2 $$ and
$$\langle M \rangle_m(h) =  \sum_{i=1}^m \mathbb{E}_{i-1}[\left(\ell(h,\z_i) - \Risk(h)\right)^2] = \sum_{i=1}^m \mathbb{E}_{\z\sim \D}[\left(\ell(h,\z) - \Risk(h)\right)^2].$$
The last equality holds because data is assumed iid. Thus, we can apply \cref{th: main_thm} to obtain with probability $1-\delta$
\[|M_m(\Q)| \leq   \frac{\operatorname{KL}(\Q,\P) +\log(2/\delta)}{\lambda } + \frac{\lambda}{2}\left([M]_m(Q)^2 + \langle M\rangle_m(Q)^2 \right) . \]
Now, we notice that $|M_m(\Q)| = m| \mathbb{E}_{h\sim \Q}[\Risk(h) - \Riskhat_{\Sm}(h)] |$  and that  for any $m,h$, because $\ell$ is nonnegative
\begin{align*}
[M]_m(h) +  \langle M\rangle_m(h) & = \sum_{i=1}^m (\ell(h,\z_i) - \Risk(h))^2 + \mathbb{E}_{\z\sim \D}[ (\ell(h,\z) - \Risk(h))^2] \\
& \leq  \sum_{i=1}^m \ell(h,\z_i)^2 + \Risk(h)^2 + \mathbb{E}_{\z\sim \D}[\ell(h,\z)^2] - \Risk(h)^2.\\
\intertext{ Thus integrating over $h$ gives: }
[M]_m(Q) +  \langle M\rangle_m(Q) & \leq \sum_{i=1}^m \mathbb{E}_{h\sim \Q} [\ell(h,\z_i)^2] + m\mathbb{E}_{h\sim \Q} [\mathrm{Quad}(h)].
\end{align*}
Then dividing by $m$ and applying the last inequality gives
\begin{multline*}
\mathbb{E}_{h\sim \Q} [\Risk(h)]  \leq  \mathbb{E}_{h\sim \Q} \left[\frac{1}{m}\sum_{i=1}^m\left(\ell(h,\z_i) + \frac{\lambda}{2} \ell(h,\z_i)^2\right)\right] \\
+ \frac{\operatorname{KL}(\Q,\P) +\log(2/\delta)}{\lambda m} + \frac{\lambda}{2}\mathbb{E}_{h\sim \Q} [\mathrm{Quad}(h)].
\end{multline*}
This concludes the proof.
\end{proof}






\subsection{Proof of \cref{th: bandits_bound}}

\begin{proof}
Let $(\lambda_m)_{i\geq 1}$ be a countable sequence of positive scalars.
As precised earlier $M_m(a):= m\left(\hat{\Delta}_m(a)-\Delta(a)\right)$ is a martingale.
We then apply \Cref{th: main_thm} with the uniform prior ($\forall a, P(a)= \frac{1}{K}$) and $\lambda= \lambda_m$  (depending possibly on $m$): with probability $1- \delta/2$, for any tuple $(m,\lambda_m)$ with $m\geq 1$, any posterior $\Q$,
\begin{align*}
\left|M_m\left(\Q\right)\right| \leq \frac{\operatorname{KL}\left(\Q, \P\right)+2 +\log (4/\delta)}{\lambda_m}+ \frac{\lambda_m}{2}\left( \hat{V}_m(\Q) + V_m(\Q) \right).
\end{align*}
Notice that for any $\Q$, $\operatorname{KL}(\Q,\P)\leq \log(K)$ by concavity of the log.
We now fix an horizon $M>0$, we then have in particular, with probability $1- \delta/2$: for any posterior $\Q$,
\begin{align*}
\left|M_m\left(Q\right)\right| \leq \frac{\log(K)+2 \log (k+1)+\log (4/\delta)}{\lambda_k}+ \frac{\lambda_m}{2}\left( \hat{V}_m(\Q) + V_m(\Q) \right).
\end{align*}
We now have to deal with $V_k(\Q), \hat{V}_k(\Q)$ for all $k\leq m$. To do so, we propose the two following lemmas.
\begin{lemma}
\label{l: bandit_lemma_1}
For all $m\geq 1$, $a\in\mathcal{A}$, $V_m(a)\leq \frac{2Cm}{\varepsilon_m}$.
Then, we have for any $m,Q$, $V_m(\Q)\leq \frac{2Cm}{\varepsilon_m}$.
\end{lemma}

\begin{proof} We have
\begin{align*}
V_t(a) &=\sum_{i=1}^m \mathbb{E}\left[\left(\left[R_i^{a^*}-R_i^a\right]-\Delta(a)\right)^2 \mid \mathcal{F}_{i-1}\right] \\
&=\sum_{i=1}^m \mathbb{E}\left[\left(R_i^{a^*}-R_i^a\right)^2 \mid \mathcal{F}_{i-1}\right]-m \Delta(a)^2\\
& \leq \sum_{i=1}^m \mathbb{E}\left[\left(R_i^{a^*}-R_i^a\right)^2 \mid \mathcal{F}_{i-1}\right]  \\
&  = \sum_{i=1}^m \mathbb{E}\left[\mathbb{E}_{A_i\sim \pi_i}\mathbb{E}_{R_i}\left[\frac{1}{\pi_i(a^*)^2} R_i(a^*)^2\mathds{1}(A_i=a^*) +\frac{1}{\pi_i(a)^2}R_i(a)^2\mathds{1}(A_i=a) \right] \mid \mathcal{F}_{i-1}\right].\\
\intertext{The last line holding because $R_i$ is independent of $\mathcal{F}_{i-1}$, $A_i$ is independent of $R_i$ and $\pi$ is $\mathcal{F}_{i-1}$ measurable.
We now use that for all $i,a$, $\mathbb{E}_{R_i}[R_i(a)^2] \leq C$ }
& = \sum_{i=1}^m \mathbb{E}\left[\mathbb{E}_{A_i\sim \pi_i}
\left[\frac{1}{\pi_i(a^*)^2} C\mathds{1}(A_i=a^*) +\frac{1}{\pi_i(a)^2}C\mathds{1}(A_i=a) \right] \mid \mathcal{F}_{i-1}\right]\\
& =\sum_{i=1}^m C\left(\frac{\pi_i(a)}{\pi_i(a)^2}+\frac{\pi_i\left(a^*\right)}{\pi_i\left(a^*\right)^2}\right) \\ &=\sum_{i=1}^m C\left(\frac{1}{\pi_i(a)}+\frac{1}{\pi_i\left(a^*\right)} \right) \\
& \leq \frac{2C m}{\varepsilon_m}.
\end{align*}
\end{proof}


\begin{lemma}
\label{l: bandit_lemma_2}
Let $m\geq 1$, with probability $1-\delta/2$, for any posterior $\Q$, we have
\[ \hat{V}_m(\Q) \leq \frac{4CKm}{\varepsilon_m\delta}. \]
\end{lemma}

\begin{proof}
Let $\Q$ a distribution over $\mathcal{A}$. Recall that
\begin{align*}
\hat{V}_m(\Q) & = \sum_{i=1}^m \left(R_i^{a^*}-R_i^a-\left[R\left(a^*\right)-R(a)\right]\right)^2 \\
& = \sum_{a\in\mathcal{A}} Q(a) \hat{V}_m(a).
\end{align*}
Notice that for any $a$, $(\hat{SM}_m^a)_m$ is a  nonnegative random variable. We then apply Markov's inequality for any $a$, with probability $1-\delta/2K$
\[ \hat{V}_m(a)\leq  \frac{2K\mathbb{E}[\hat{V}_m(a)]}{\delta} .  \]
Noticing that $\mathbb{E}[\hat{V}_m(a)] = \mathbb{E}[V_m(a)]$, we can apply \cref{l: bandit_lemma_1} to conclude that
$$\mathbb{E}[\hat{V}_m(a)] \leq \frac{2Cm}{\varepsilon_m}.$$
Finally, taking an union bound on thoser events for all $a\in\mathcal{A}$ gives us, with probability $1-\delta/2$, for any posterior $\Q$
\begin{align*}
V_m(\Q) & \leq \sum_{a\in\mathcal{A}} Q(a) \hat{V}_m(a) \\
& \leq \sum_{a\in\mathcal{A}} Q(a) \frac{4CKm}{\varepsilon_m\delta} \\
&= \frac{4CKm}{\varepsilon_m\delta}.
\end{align*}
This concludes the proof.
\end{proof}
To conclude, we apply \cref{l: bandit_lemma_1,l: bandit_lemma_2} to get that with probability $1-\delta$, for any posterior $\Q$
\begin{align*}
\left|M_m\left(\Q\right)\right| & \leq \frac{\operatorname{KL}\left(\Q, \P\right) +\log (4/\delta)}{\lambda_m}+ \frac{Cm\lambda_m}{\varepsilon_m} \left(1+ \frac{2K}{\delta}    \right).
\end{align*}
Dividing by $m$ and taking $$\lambda_m= \sqrt{\frac{\left(\log(K) +\log (4/\delta)\right) \varepsilon_m}{Cm\left(1+ \frac{2K}{\delta}    \right)}}$$ concludes the proof.

\end{proof}





\subsection{Proof of Cor. \ref{cor: bound_mart}}


\begin{proof}
Fix $\delta>0$. For any pair $(\lambda_k,P_k), k\geq 1$, we apply \Cref{th: main_thm} with $$\delta_k := \frac{\delta}{k(k+1)} \geq \frac{\delta}{(k+1)^2}. $$
Notice that we have $\sum_{k=1}^{+\infty} \delta_k = \delta$.
We then have with probability $1-\delta_k$ over $S$, for any $m\geq 1$, any posterior $\Q$,
\[ \left|M_m\left(\Q\right)\right| \leq \frac{\KL\left(\Q, \P_k\right)+2 \log (k+1)+\log (2/\delta)}{\lambda_k}+ \frac{\lambda_k}{2}\left( \hat{V}_m(\Q) + V_m(\Q) \right).\]
Taking an union bound on all those event, gives the final result, valid with probability $1-\delta$ over the sample $S$, for any any tuple $(m,\lambda_k,P_k)$ with $m,k\geq 1$, any posterior $\Q$ over $\mathcal{H}$. This gives \Cref{eq: bound_mart_1}.

To obtain \cref{eq: bound_mart_2}, we restrict the range of \cref{eq: bound_mart_1} to the tuples $(m,\lambda_m,P_m), m\geq 1$ (the restricted set of tuples where $k=m$) and we bound both $\hat{V}_m(\Q),V_m(\Q)$ by $\sum_{i=1}^m C_i^2$ to conclude.
\end{proof}

\subsection{Proof of Cor. \ref{cor: bounded_case}}


\begin{proof}
For the first bound we start from the intermediary result \cref{eq: intermediary_result_main} of  \cref{th: main_thm}. Using the same marrtingale as in \cref{th: main_thm_iid} gives, for any $\eta\in\mathbb{R}$, holding with probability $1-\delta$ for any $m>0,Q\in\mathcal{M}(\mathcal{H})$
\begin{multline*}
\eta \left(\sum_{i=1}^m \mathbb{E}_{h\sim \Q}[\ell(h,\z_i)]  -m \mathbb{E}_{h\sim \Q}[ \Risk(h) ] \right) \\
\leq  \operatorname{KL}(\Q,\P) +\log(1/\delta) + \frac{\eta^2}{2}\sum_{i=1}^m \mathbb{E}_{h\sim \Q}[\Delta[M]_i(h) + \Delta \langle M\rangle_i(h) ].
\end{multline*}
Taking $\eta= \pm\lambda$ with $\lambda>0$ gives
\begin{align}
\label{eq: temp_result}
\lambda m\left | \mathbb{E}_{h\sim \Q}[ \Risk(h) -\Riskhat_{\Sm}(h)] \right | & \leq \operatorname{KL}(\Q,\P) +\log(1/\delta) \\
& + \frac{\lambda^2}{2}\sum_{i=1}^m \mathbb{E}_{h\sim \Q}[\Delta[M]_i(h) + \Delta \langle M\rangle_i(h) ].
\end{align}
Finally, divide by $\lambda m$ and bound $\Delta[M]_i(h) + \Delta \langle M\rangle_i(h)$ by $2K^2$ to conclude.


For the second bound, we start from \Cref{eq: temp_result} again and for a fixed $m$, we now apply our result with $\lambda'=\lambda/m$. We then have for any $m$, with probability $1-\delta$, for any $\Q$
\[ \lambda\left |  \mathbb{E}_{h\sim \Q}[ \Risk(h) -\Riskhat_{\Sm}(h)] \right |  \leq \operatorname{KL}(\Q,\P) +\log(1/\delta) + \frac{\lambda^2}{2m^2}\sum_{i=1}^m \mathbb{E}_{h\sim \Q}[\Delta[M]_i(h) + \Delta \langle M\rangle_i(h) ].\]
Finally, dividing by $\lambda$, bounding $\Delta[M]_i(h) + \Delta \langle M\rangle_i(h)$ by $2K^2$ and rearranging the terms concludes the proof.
\end{proof}


\end{noaddcontents}