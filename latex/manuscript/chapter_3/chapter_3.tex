\chapter[Mitigating Initialisation Impact by Real-Time Control: Online PAC-Bayes Learning]{Mitigating Initialisation Impact by Real-Time Control: Online PAC-Bayes Learning}
\label{chap:online-pb}

\addchapterlof
\addchapterloa
\addchapterloe

\vspace{-1.0cm}
\begin{center}
\textbf{This chapter is based on the following papers}\\[0.1cm]
\end{center}
\printpublication{haddouche2022online}
\\
\printpublication{haddouche2023pac}
\\
\printpublication{viallard2023learning}

\vspace{0.2cm}
\minitoc

\begin{abstract}
Put OPB here. Precise in the intro that the martingale bounds allow to go beyond batch learning but that this has never been made for OL. Put the supermartingale OPB bound in a supplementary section and the Online WPB bound after the main results of OPB to reach heavy-tailed losses. 
\end{abstract}


\section{Online PAC-Bayes learning beyond bounded losses.}
\label{sec: main_result_onl}

Recently, an online learning framework has been designed in \citet{haddouche2022online}. This allowed the design of Online PAC-Bayes (OPB) algorithms which involved the use of history-dependent priors evolving at each time step of the learning procedure. The main contribution of this section is an OPB bound valid for unbounded losses.

\paragraph{Framework} We consider the same framework as in \Cref{sec: iid_case} except we do not make any assumption on the data distribution. Our goal is now to define a posterior sequence $(\Q_i)_{i\geq 1}$ from a prior sequence $(\P_i)_{i\geq 1}$. We also define a filtration $(\mathcal{F}_{i})_{i\geq 1}$ adapted to $(z_i)_{i\geq 1}$. We reuse the following definitions extracted from \cite{haddouche2022online}.

\paragraph{Definitions} For all $i$, we denote by $\mathbb{E}_{i}[.]$ the conditional expectation $\mathbb{E}[.\mid \mathcal{F}_i]$.

A \emph{stochastic kernel} from $\cup_{m=1}^\infty\mathcal{Z}^m$ to $\mathcal{H}$ is defined as a mapping $Q: \cup_{m=1}^\infty\mathcal{Z}^m\times \Sigma_{\mathcal{H}} \rightarrow [0,1]$ where
(i) For any $B\in \Sigma_{\mathcal{H}}$, the function  $S\mapsto Q(S,B)$ is measurable,  (ii) For any $\S$, the function $B\mapsto Q(S,B)$ is a probability measure over $\mathcal{H}$.


We also say that a sequence of stochastic kernels $(P_i)_{i\geq 1}$ is an \emph{online predictive sequence} if (i) for all $i\geq 1, S\in\cup_{m=1}^\infty\mathcal{Z}^m, P_i(S,.)$ is $\mathcal{F}_{i-1}$ measurable and (ii) for all $i \geq 2$, $P_i(S,.)\gg P_{1}(S,.)$.

\textbf{Main result.} We now state the main theorem of this section, which extends the remits of the Online PAC-Bayes framework to the case of unbounded losses.

\begin{theorem}
  \label{th: main_thm_onl}
  For any distribution over the (countable) dataset $\S$, any $\lambda>0$ and any online predictive sequence (used as priors) $(P_i)_{i\geq 1}$, we have with probability at least $1-\delta$ over the sample $S\sim\mu$, the following, holding for the data-dependent measures $P_{i,S}:= P_i(S,.)$ any posterior sequence $(Q_i)_{i\geq 1}$ and any $m\geq 1$:

  \begin{multline*}
     \sum_{i=1}^m \mathbb{E}_{h_i\sim Q_{i}}\left[ \mathbb{E}[\ell(h_i,z_i) \mid \mathcal{F}_{i-1}]    \right]  \leq \sum_{i=1}^m \mathbb{E}_{h_i\sim Q_{i}}\left[ \ell(h_i,z_i) \right] +\frac{\lambda}{2}\sum_{i=1}^m \mathbb{E}_{h_i\sim Q_i}\left[ \hat{V}_i(h_i,z_i) + V_i(h_i) \right] \\
     + \sum_{i=1}^m\frac{\operatorname{KL}(Q_{i}\| P_{i,S})}{\lambda}  + \frac{\log(1/\delta)}{\lambda}.
  \end{multline*}
  With for all $i$, $\hat{V}_i(h_i,z_i)= (\ell(h_i,z_i)-\mathbb{E}_{i-1}[\ell(h_i,z_i)])^2$ is the empirical variance at time $i$ and $V_i(h_i)= \mathbb{E}_{i-1}[\hat{V}(h_i,z_i)]$ is the true conditional variance.
\end{theorem}

Proof lies in \Cref{sec: proof_main_thm_online}.

\textbf{Analysis of the bound.} This bound is, to our knowledge, the first Online PAC-Bayes bound in literature holding for unbounded losses. It is semi-empirical as the variance and empirical variance terms have theoretical components. However, these terms can be controlled with assumptions on conditional second-order moments and not on exponential ones (as made in \citealp{haddouche2022online} where the bounded loss assumption was used to obtain conditional subgaussianity). To emphasise our point, we consider as in \Cref{sec: iid_case} the case of the quadratic loss $\ell(h,z)= (h-z)^2$. Here, we only need to assume that our data have a finite variance if we restrict our posteriors to have both bounded means and variance. Also the meaning of the online predictive sequence $P_i$ is that we must be able to design properly a sequence of priors before drawing our data, this can be for instance an online algorithm whihc generate a prior distribution from past data at each time step.

Finally, we note that if we assume being able to bound simultaneaously all condtional means and variance (which is strictly less restrictive than bounding the loss),then  \cref{th: main_thm_onl} suggests a new online learning objective which is an online counterpart to \Cref{eq: optim_obj}.

\begin{align}
    \forall i\geq1\; \hat{Q}_{i+1}&= \underset{Q\in\mathcal{M}^+_1(\mathcal{H})}{\mathrm{argmin}} \mathbb{E}_{h_i\sim Q} \; \left[\ell(h_i,z_i)+ \frac{\lambda}{2}\ell(h_i,z_i)^2\right] + \frac{\operatorname{KL}(Q\| P_{i,S})}{\lambda}
\end{align}

\textbf{Comparison with literature.} Our most natural comparison point is Theorem 2.3 of \cite{haddouche2022online} (re-stated in \cref{sec: pac_b_background}). We claim that \Cref{th: main_thm_onl} is a strict improvement of their result on various sides described below.

\begin{itemize}
  \item If we assume our loss to be bounded, then we can upper bound our empirical/theoretical variance terms to recover exactly \citet[][Theorem 2.3]{haddouche2022online}. Our bound can then be seen as a strict extension of theirs and shows that bounding order two moments is a sufficient condition to perform online PAC-Bayes: subgaussianity induced by boundedness is not necessary even when our data are non iid.
  \item Another crucial point lies on the range of our result which holds with high probability for any countable posterior sequence $(Q_i)_{i\geq 1}$, any time $m$ and the priors $(P_{i,S})_{i\geq 1}$.
  This is far much general than \citet[][Theorem 2.3]{haddouche2022online} which holds only for a single $m$ and a single posterior sequence $(Q_{i,S})_{i=1..m}$. This happens because in \citet{haddouche2022online}, the change of measure inequality has not been exploited: they used a preliminary theorem from \citet{rivasplata2020pac} which holds for a single (data-dependent) prior/posterior couple. This preliminary theorem already involved Markov's inequality which forced the authors to assume conditionnal subgaussianity to deal with an exponential moment. On the contrary, we exploited the fact that our online predictive sequence was history-dependent to use the change of measure inequality at any time step and control an exponential supermartingale through Ville's inequality.
  \item In \citet[Eq. 1]{haddouche2022online}, an OPB algorithm is given by their upper bound. This works because their associated learning objective admits a close form (Gibbs posterior) which matches the fact their bound hold for a single posterior sequence. Because our bound holds uniformly on all posteriors, it is now legitimate to restrict their algorithms to any parametric class of distributions and perform any optimisation algorithm to obtain a surrogate of the best candidate.
\end{itemize}

 Online PAC-Bayes as presented in \citet{haddouche2022online} relies on a conditional subgaussiannity assumption to control an exponential moment. They did not exploit a martingale-type structure to do so. Our supermartingale approach has proven to be well suited to Online PAC-Bayes as we provided atheorem valid for unbounded losses holding simultaneously on all posteriors: two points which have not been reached in \citet{haddouche2022online}.

\subsection{Proof of \Cref{th: main_thm_onl}}
\label{sec: proof_main_thm_online}
 \begin{proof}
   We fix $m\geq 1$, $\S$ a countable dataset and $(P_i)_{i\geq 1}$ an online predictive sequence. We aim to design a $m$-tuple of probabilities. Thus, our predictor set of interest is $\mathcal{H}_m:= \mathcal{H}^{\otimes m}$ and then, our predictor $h$ is a tuple $(h_1,..,h_m)\in\mathcal{H}$.

   Our goal is to apply the change of measure inequality on $\mathcal{H}_m$ to a specific function $f_m$ inspired from Lemma \ref{l: bercu_touati}. We define this function below, for any sample $\S$ and any predictor $h^m=(h_1,...,h_m)$

   \begin{align*}
   f_m(S,h^m) & := \sum_{i=1}^m \lambda X_i(h_i,z_i)  - \frac{\lambda^2}{2}\sum_{i=1}^m(\hat{V}_i(h_i,z_i) + V_i(h_i)),
   \end{align*}
   where $X_i(h_i,z_i)= \mathbb{E}_{i-1}[\ell(h_i,z_i)]- \ell(h_i,z_i)$. Notice that for fixed $h$, the sequence $(f_m(\S,h))_{m\geq 1}$ is a supermartingale according to Lemma \ref{l: bercu_touati}.

   Now for a given posterior tuple $Q_1,...Q_m$ we define $Q= Q_1 \otimes ...\otimes Q_m$ and also $P^m_S = P_{1,S}\otimes...\otimes P_{m,S}$. We can now properly apply the change of measure inequality for any $m$:
   \begin{align*}
    \sum_{i=1}^m \mathbb{E}_{h_i\sim Q_i}[\lambda X_i(h_i,z_i)  - \frac{\lambda^2}{2}(\hat{V}_i(h_i,z_i) + V_i(h_i))] & = \mathbb{E}_{h^m\sim Q}\left[ f_m(S,h^m) \right] \\
    & \leq \operatorname{KL}(Q,P^m_S) + \log \left( \mathbb{E}_{h^m\sim P^m_S}\exp(f_m(S,h^m))  \right).
   \end{align*}

   Noticing that $\operatorname{KL}(Q,P^m_S)= \sum_{i=1}^m \operatorname{KL}(Q_i,P_{i,S})$, the only remaining term to deal with is the exponential rv.

   To do so we prove the following lemma:

   \begin{lemma}
     The sequence $(M_m:=\mathbb{E}_{h^m\sim P^m_S}\exp(f_m(S,h^m))_{m\geq 1}$ is a non-negative supermartingale.
   \end{lemma}
   \begin{proof}
   We fix $m\geq 1$ and we recall that for any $i$, $P_{i,S}$ is $\mathcal{F}_{i-1}$-measurable. We show that $\mathbb{E}_{m-1}[M_m] \leq M_{m-1}$. We first recover $M_{m-1}$ from $\mathbb{E}_{m-1}[M_m]$.

     \begin{align*}
       \mathbb{E}_{m-1}[M_m]& =\mathbb{E}_{m-1}\left[\mathbb{E}_{h^m\sim P^m_S}\exp(f_m(S,h^m)\right] \\
       & = \mathbb{E}_{m-1}\left[\mathbb{E}_{h_1,..,h_m\sim P_{1,S}\otimes...\otimes P_{m,S}}\exp(f_m(S,h^m)\right] \\
       & = \mathbb{E}_{m-1}\left[\mathbb{E}_{h_1,..,h_m\sim P_{1,S}\otimes...\otimes P_{m,S}}\left[\Pi_{i=1}^m\exp\left(\lambda X_i(h_i,z_i)  - \frac{\lambda^2}{2}(\hat{V}_i(h_i,z_i) + V_i(h_i))\right)\right] \right] \\
        & = M_{m-1} \mathbb{E}_{m-1}\left[ \mathbb{E}_{h_m\sim P_{m,S}}\left[\exp\left(\lambda X_m(h_m,z_m)  - \frac{\lambda^2}{2}(\hat{V}_m(h_m,z_m) + V_m(h_m))\right) \right]\right].
     \end{align*}
 The last line holding because $P^{m-1}_S = P_{1,S}\otimes...\otimes P_{m-1,S}$ is $\mathcal{F}_{m-1}$ measurable.


   Now we exploit the fact that $P_{m,S}$ is $\mathcal{F}_{m-1}$ measurable to apply a conditional Fubini lemma stated in \citet[][Lemma D.3]{haddouche2022online}. We have:

   \begin{multline*}
     \mathbb{E}_{m-1}\left[ \mathbb{E}_{h_m\sim P_{m,S}}\left[\exp\left(\lambda X_m(h_m,z_m)  - \frac{\lambda^2}{2}(\hat{V}_m(h_m,z_m) + V_m(h_m))\right) \right]\right] \\ =  \mathbb{E}_{h_m\sim P_{m,S}}\left[\mathbb{E}_{m-1}\left[\exp\left(\lambda X_m(h_m,z_m)  - \frac{\lambda^2}{2}(\hat{V}_m(h_m,z_m) + V_m(h_m))\right) \right]\right].
   \end{multline*}

 Now we can apply Lemma \ref{l: bercu_touati} for any $h_m\in\mathcal{H}$ with $\Delta M_{m}=X_m(h_m,z_m), \Delta[M]_{m}=\hat{V}(h_m,z_m)$ and $\Delta\langle M\rangle_{m}= V_m(h_m)$. We then have for all $h_m\in\mathcal{H}$:

 \[ \mathbb{E}_{m-1}\left[\exp\left(\lambda X_m(h_m,z_m)  - \frac{\lambda^2}{2}(\hat{V}_m(h_m,z_m) + V_m(h_m))\right) \right] \leq 1.  \]

 Thus $\mathbb{E}_{m-1}[M_m] \leq M_{m-1}$, this concludes the lemma's proof.
   \end{proof}

 Now we can apply Ville's inequality which implies that with probability at least $1-\delta$, for any $m\geq 1$:

 \[ \mathbb{E}_{h^m\sim P^m_S}\exp(f_m(S,h^m)) \leq \frac{1}{\delta}. \]

 Thus we have with probability at least $1-\delta$, for any posterior sequence $(Q_i)_{i\geq 1}$, the data-dependent measures $P_{1,S},...,P_{m,S}$ and any $m\geq 1$:

 \begin{align*}
  \sum_{i=1}^m \mathbb{E}_{h_i\sim Q_i}\left[\lambda X_i(h_i,z_i)  - \frac{\lambda^2}{2}(\hat{V}_i(h_i,z_i) + V_i(h_i))\right] \leq \sum_{i=1}^m \operatorname{KL}(Q_i,P_{i,S}) + \log \left( \frac{1}{\delta}  \right).
 \end{align*}

 Re-organising the terms in this bound and dividing by $\lambda$ concludes the proof.

 \end{proof}

\newpage

